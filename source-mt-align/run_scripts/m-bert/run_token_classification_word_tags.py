# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" Fine-tuning the library models for named entity recognition on CoNLL-2003. """

NOTE = \
'''
    This script is modified from huggingface/transformer's run_ner.py.
    To put it in a simple way, it is a script for token classification, and specifically word-wise word tag prediction.

    
    A typical composition of arguments is like this:
    --model_name_or_path model --do_train --source_text xxx --mt_text xxx --source_tags xxx 
    --mt_word_tags xxx --mt_gap_tags xxx --learning_rate 3e-5 --max_seq_length 384 --output_dir output --cache_dir output 
    --save_steps 1000 --num_train_epochs 5.0 --overwrite_cache --overwrite_output_dir
    
    Some newly added arguments are:
    --source_text FILE
    --mt_text FILE
    --source_tags FILE    [only required in training]
    --mt_word_tags FILE    [only required in training]
    --mt_gap_tags FILE    [only requried in training]
    --valid_tags FILE    [if not set, OK/BAD is the default valid tags.]
    
    --source_prob_threshold FLOAT    [only required in testing]
    --mt_word_prob_threshold FLOAT    [only required in testing]
    --mt_gap_prob_threshold FLOAT    [only required in testing]
    --tag_prob_pooling [mean,max,min]   [set the mode for pooling several token tags during prediction]
    --bad_loss_lambda FLOAT    [only optional in training]
'''

import datetime
import logging
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

import numpy as np

from transformers import (
    AutoConfig,
    # AutoModelForTokenClassification,
    AutoTokenizer,
    # EvalPrediction,
    HfArgumentParser,
    # Trainer,
    TrainingArguments,
    set_seed,
)

logger = logging.getLogger(__name__)

#############################################################
#
# Start:
# Some of classes and functions about data defined by myself
#
#############################################################

from pathlib import Path
from torch.utils.data import Dataset
from transformers.data import DataProcessor


def map_offset(origin_text, tokenizer):
    '''
    A very lovely method helps to generate an offset mapping dictionary between original tokens of a sentence
    and the tokens generated by BertTokenizer(or other etc.)
    Made special adaption for punctuations like hyphens.
    '''

    orig_tokens = origin_text.split()
    pieced_tokens = []
    for token in tokenizer.basic_tokenizer.tokenize(
            origin_text, never_split=tokenizer.all_special_tokens):
        wp_token = tokenizer.wordpiece_tokenizer.tokenize(token)
        if '[UNK]' in wp_token:    # append the original token rather than UNK to avoid match error
            assert len(wp_token) == 1, f'Token {token} is splited by wordpiece but still contains UNK??'
            pieced_tokens.append(token)
        else:
            pieced_tokens.extend(wp_token)

    mapping = {}
    pieced_i = 0

    for orig_i, orig_token in enumerate(orig_tokens):
        tmp_token = pieced_tokens[pieced_i]

        # normalize orig_token (lowercase, accent-norm. No punc-split-norm)
        if orig_token not in tokenizer.all_special_tokens \
                and orig_token not in tokenizer.basic_tokenizer.never_split:
            # special tokens needs no normalization
            if tokenizer.basic_tokenizer.do_lower_case:
                orig_token = tokenizer.basic_tokenizer._run_strip_accents(orig_token)
                orig_token = orig_token.lower()

        # match!
        while True:
            mapping[pieced_i] = orig_i
            pieced_i += 1
            if tmp_token == orig_token:
                break
            else:
                tmp_token += pieced_tokens[pieced_i].replace('##', '')

            if len(tmp_token) > len(orig_token):    # error raising
                msg = f'Original Text:  {" ".join(orig_tokens)}\n' \
                      f'Pieced Text: {" ".join(pieced_tokens)}\n' \
                      f'Original Token: {orig_token}\n' \
                      f'Pieced Tmp Token: {tmp_token}\n' \
                      f'Mapping: {mapping}'
                raise ValueError('Maybe original tokens and pieced tokens does not match.\n' + msg)

    return mapping


def generate_source_and_mt_tag_mask(token_type_ids):
    '''
    generate masks for source tags and MT tags. Note that CLS and SEP are not included.
    specifically, identifying the min and max indices of 1 in token_type_ids, which respectively identifies the
    first token of MT text and the SEP token of MT text.
    '''
    batch_size, max_len = token_type_ids.shape
    source_tag_masks = []
    mt_tag_masks = []
    mt_gap_tag_masks = []
    for i in range(batch_size):
        row_type_ids = token_type_ids[i, :]
        _mt_indices = row_type_ids.nonzero().view(-1)
        min_i, max_i = _mt_indices.min(), _mt_indices.max()

        source_tag_mask = torch.zeros_like(row_type_ids)
        source_tag_mask[1:min_i - 1] = 1  # CLS and SEP for source text are excluded

        mt_tag_mask = torch.zeros_like(row_type_ids)
        mt_tag_mask[min_i:max_i] = 1  # SEP for MT text is excluded

        mt_gap_tag_mask = torch.zeros_like(row_type_ids)
        mt_gap_tag_mask[min_i:max_i+1] = 1    # SEP for MT text is included because gaps are one more than MT words

        source_tag_masks.append(source_tag_mask.type(torch.bool))
        mt_tag_masks.append(mt_tag_mask.type(torch.bool))
        mt_gap_tag_masks.append(mt_gap_tag_mask.type(torch.bool))

    source_tag_masks = torch.stack(source_tag_masks, dim=0)
    mt_tag_masks = torch.stack(mt_tag_masks, dim=0)
    mt_gap_tag_masks = torch.stack(mt_gap_tag_masks, dim=0)

    res = (source_tag_masks, mt_tag_masks,
           mt_gap_tag_masks)

    return res


@dataclass
class QETagClassificationInputExample:
    guid: str
    source_text: str
    mt_text: str
    source_tags: Optional[str] = None
    mt_word_tags: Optional[str] = None
    mt_gap_tags: Optional[str] = None


@dataclass
class QETagClassificationInputFeature:
    input_ids: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    word_tag_labels: Optional[List[int]] = None
    gap_tag_labels: Optional[List[int]] = None


class QETagClassificationProcessor(DataProcessor):
    def __init__(self, args):
        self.source_text = args.source_text
        self.mt_text = args.mt_text
        self.source_tags = args.source_tags
        self.mt_word_tags = args.mt_word_tags
        self.mt_gap_tags = args.mt_gap_tags

    def get_examples(self, set_type):

        def read_f(fn):
            with Path(fn).open(encoding='utf-8') as f:
                return [l.strip() for l in f]

        src_lines = read_f(self.source_text)
        mt_lines = read_f(self.mt_text)

        assert len(src_lines) == len(mt_lines), 'Inconsistent number of line'

        if set_type == 'train':
            assert self.source_tags is not None, 'You need to specify source QE Tags file to do train.'
            assert self.mt_word_tags is not None, 'You need to specify MT QE Tags file to do train.'
            source_tags_lines = read_f(self.source_tags)
            mt_word_tags_lines = read_f(self.mt_word_tags)
            assert len(src_lines) == len(source_tags_lines), 'Inconsistent number of line'
            assert len(src_lines) == len(mt_word_tags_lines), 'Inconsistent number of line'

            if self.mt_gap_tags is not None:
                mt_gap_tags_lines = read_f(self.mt_gap_tags)
                assert len(src_lines) == len(mt_gap_tags_lines)
            else:
                mt_gap_tags_lines = [None] * len(src_lines)

        elif set_type == 'eval':
            source_tags_lines = mt_word_tags_lines = mt_gap_tags_lines = [None] * len(src_lines)

        else:
            raise ValueError(f'Invalid set type {set_type}')

        i = 0
        examples = []
        for src_line, mt_line, source_tags_line, mt_word_tags_line, mt_gap_tags_line in zip(src_lines, mt_lines, source_tags_lines,
                                                                           mt_word_tags_lines, mt_gap_tags_lines):
            guid = f'{set_type}-{i}'
            examples.append(
                QETagClassificationInputExample(guid=guid, source_text=src_line, mt_text=mt_line,
                                                source_tags=source_tags_line,
                                                mt_word_tags=mt_word_tags_line,
                                                mt_gap_tags=mt_gap_tags_line)
            )
            i += 1

        return examples


class QETagClassificationDataset(Dataset):
    def __init__(self, args, tokenizer, set_type, label_to_id):
        self.tokenizer = tokenizer
        self.processor = QETagClassificationProcessor(args)

        if args.source_tags is not None:
            assert args.mt_word_tags is not None, 'You must specify MT QE tags simultaneously'
        else:
            assert args.mt_word_tags is None, 'You must specify Source QE tags simultaneously.'

        if args.mt_gap_tags is not None:
            assert args.mt_word_tags is not None, 'You must specify MT Word tags with MT Gap tags.'

        msg = f"Creating features from dataset files: {args.source_text}, {args.mt_text}"
        if args.source_tags is not None and args.mt_word_tags is not None:
            msg += f', {args.source_tags}, {args.mt_word_tags}'
        if args.mt_gap_tags is not None:
            msg += f', {args.mt_gap_tags}'
        logger.info(msg)

        examples = self.processor.get_examples(set_type)

        batch_text_encoding = tokenizer(
            [(e.source_text, e.mt_text) for e in examples],
            max_length=args.max_seq_length,
            padding="max_length",
            truncation=True,
        )

        if set_type == 'train':
            qe_tag_map = label_to_id
            id_to_label = {i: label for label, i in label_to_id.items()}
            DEF_TAG = id_to_label[0]
            batch_word_tag_encoding = []
            batch_gap_tag_encoding = []
            for i, e in enumerate(examples):
                source_tags = e.source_tags.split()
                mt_word_tags = e.mt_word_tags.split()
                if e.mt_gap_tags is not None:
                    mt_gap_tags = e.mt_gap_tags.split()
                    assert len(mt_gap_tags) == len(mt_word_tags) + 1, 'Gap tags should always be one more than the word tags!'
                else:
                    mt_gap_tags = None

                # default QE tag for CLS and SEP are DEFAULT TAG
                qe_tag_encoding = [DEF_TAG] + source_tags + [DEF_TAG] + mt_word_tags + [DEF_TAG]

                origin_text = f'{tokenizer.cls_token} {e.source_text} {tokenizer.sep_token} {e.mt_text} ' \
                              f'{tokenizer.sep_token}'
                pieced_to_origin_mapping = map_offset(origin_text, tokenizer)
                max_pieced_token_len = max(pieced_to_origin_mapping.keys()) + 1
                pieced_qe_tag_encoding = [qe_tag_encoding[pieced_to_origin_mapping[k]] for k in
                                          range(max_pieced_token_len)]
                qe_tag_encoding = pieced_qe_tag_encoding

                while len(qe_tag_encoding) < args.max_seq_length:  # padding adaption
                    qe_tag_encoding.append(DEF_TAG)  # PADs' tag does not really influence for the mask

                if mt_gap_tags is not None:
                    # for convenience we add source tags here but actually they are not effective for the mask
                    qe_gap_tag_encoding = [DEF_TAG] + source_tags + [DEF_TAG] + mt_gap_tags
                    pieced_qe_gap_tag_encoding = [qe_gap_tag_encoding[pieced_to_origin_mapping[k]] for k in
                                                  range(max_pieced_token_len)]
                    qe_gap_tag_encoding = pieced_qe_gap_tag_encoding

                    while len(qe_gap_tag_encoding) < args.max_seq_length:
                        qe_gap_tag_encoding.append(DEF_TAG)
                else:
                    qe_gap_tag_encoding = []

                if max(len(qe_tag_encoding), len(qe_gap_tag_encoding)) > args.max_seq_length:
                    # seems source and mt will be truncated respectively to fit the max_seq_length requirement
                    # so it is hard to map offset in that case.
                    # raise ValueError(
                    #     'I have not done the adaption to qe_tags_input when the text input exceeds max length')
                    continue


                # transfer tag index to one hot labels
                num_label = len(qe_tag_map)
                # (DEPRECATED)
                # if num_label > 2:
                #     example_label = [[0] * num_label for _ in range(len(qe_tag_encoding))]
                #     for i in range(len(qe_tag_encoding)):
                #         qe_tag = qe_tag_encoding[i]
                #         assert qe_tag in qe_tag_map, f'{qe_tag} is an invalid tag among {",".join(qe_tag_map.keys())}'
                #         example_label[i][qe_tag_map[qe_tag_encoding[i]]] = 1
                # else:
                word_tag_labels = []
                for t in qe_tag_encoding:
                    assert t in qe_tag_map, f'{t} is an invalid tag among {",".join(qe_tag_map.keys())}'
                    word_tag_labels.append(qe_tag_map[t])

                if len(qe_gap_tag_encoding) > 0:
                    gap_tag_labels = []
                    for t in qe_gap_tag_encoding:
                        assert t in qe_tag_map, f'{t} is an invalid tag among {",".join(qe_tag_map.keys())}'
                        gap_tag_labels.append(qe_tag_map[t])
                else:
                    gap_tag_labels = None

                batch_word_tag_encoding.append(word_tag_labels)
                batch_gap_tag_encoding.append(gap_tag_labels)

        else:
            batch_word_tag_encoding = batch_gap_tag_encoding = [None for _ in examples]

        self.features = []
        for i in range(len(batch_word_tag_encoding)):
            text_inputs = {k: batch_text_encoding[k][i] for k in batch_text_encoding}
            word_tag_labels = batch_word_tag_encoding[i]
            gap_tag_labels = batch_gap_tag_encoding[i]
            feature = QETagClassificationInputFeature(input_ids=text_inputs['input_ids'],
                                                      attention_mask=text_inputs['attention_mask'],
                                                      token_type_ids=text_inputs['token_type_ids'],
                                                      word_tag_labels=word_tag_labels,
                                                      gap_tag_labels=gap_tag_labels)
            self.features.append(feature)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i) -> QETagClassificationInputFeature:
        return self.features[i]


#############################################################
#
#  End
#
#############################################################


#############################################################
#
# Start:
# Some of classes and functions about model defined by myself
#
#############################################################
import collections
import math
import torch
import warnings
from torch import nn
from torch.utils.data import DataLoader
from torch.nn.modules.loss import CrossEntropyLoss, BCELoss
from transformers.modeling_bert import BertModel, BertPreTrainedModel, BertEmbeddings, BertEncoder, BertPooler
from transformers.modeling_outputs import BaseModelOutputWithPooling
from transformers.trainer import Trainer
from transformers.optimization import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm
from typing import Union, Any, NamedTuple, List, Tuple, Optional

'''
====================================================================================
  @wyzypa
  20201101 definition of CRF copied from AllenNLP
====================================================================================
'''
VITERBI_DECODING = Tuple[List[int], float]  # a list of tags, and a viterbi score


def logsumexp(tensor: torch.Tensor, dim: int = -1, keepdim: bool = False) -> torch.Tensor:
    """
    A numerically stable computation of logsumexp. This is mathematically equivalent to
    `tensor.exp().sum(dim, keep=keepdim).log()`.  This function is typically used for summing log
    probabilities.

    # Parameters

    tensor : `torch.FloatTensor`, required.
        A tensor of arbitrary size.
    dim : `int`, optional (default = `-1`)
        The dimension of the tensor to apply the logsumexp to.
    keepdim: `bool`, optional (default = `False`)
        Whether to retain a dimension of size one at the dimension we reduce over.
    """
    max_score, _ = tensor.max(dim, keepdim=keepdim)
    if keepdim:
        stable_vec = tensor - max_score
    else:
        stable_vec = tensor - max_score.unsqueeze(dim)
    return max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()


def viterbi_decode(
        tag_sequence: torch.Tensor,
        transition_matrix: torch.Tensor,
        tag_observations: Optional[List[int]] = None,
        allowed_start_transitions: torch.Tensor = None,
        allowed_end_transitions: torch.Tensor = None,
        top_k: int = None,
):
    """
    Perform Viterbi decoding in log space over a sequence given a transition matrix
    specifying pairwise (transition) potentials between tags and a matrix of shape
    (sequence_length, num_tags) specifying unary potentials for possible tags per
    timestep.

    # Parameters

    tag_sequence : `torch.Tensor`, required.
        A tensor of shape (sequence_length, num_tags) representing scores for
        a set of tags over a given sequence.
    transition_matrix : `torch.Tensor`, required.
        A tensor of shape (num_tags, num_tags) representing the binary potentials
        for transitioning between a given pair of tags.
    tag_observations : `Optional[List[int]]`, optional, (default = `None`)
        A list of length `sequence_length` containing the class ids of observed
        elements in the sequence, with unobserved elements being set to -1. Note that
        it is possible to provide evidence which results in degenerate labelings if
        the sequences of tags you provide as evidence cannot transition between each
        other, or those transitions are extremely unlikely. In this situation we log a
        warning, but the responsibility for providing self-consistent evidence ultimately
        lies with the user.
    allowed_start_transitions : `torch.Tensor`, optional, (default = `None`)
        An optional tensor of shape (num_tags,) describing which tags the START token
        may transition *to*. If provided, additional transition constraints will be used for
        determining the start element of the sequence.
    allowed_end_transitions : `torch.Tensor`, optional, (default = `None`)
        An optional tensor of shape (num_tags,) describing which tags may transition *to* the
        end tag. If provided, additional transition constraints will be used for determining
        the end element of the sequence.
    top_k : `int`, optional, (default = `None`)
        Optional integer specifying how many of the top paths to return. For top_k>=1, returns
        a tuple of two lists: top_k_paths, top_k_scores, For top_k==None, returns a flattened
        tuple with just the top path and its score (not in lists, for backwards compatibility).

    # Returns

    viterbi_path : `List[int]`
        The tag indices of the maximum likelihood tag sequence.
    viterbi_score : `torch.Tensor`
        The score of the viterbi path.
    """
    if top_k is None:
        top_k = 1
        flatten_output = True
    elif top_k >= 1:
        flatten_output = False
    else:
        raise ValueError(f"top_k must be either None or an integer >=1. Instead received {top_k}")

    sequence_length, num_tags = list(tag_sequence.size())

    has_start_end_restrictions = (
            allowed_end_transitions is not None or allowed_start_transitions is not None
    )

    if has_start_end_restrictions:

        if allowed_end_transitions is None:
            allowed_end_transitions = torch.zeros(num_tags)
        if allowed_start_transitions is None:
            allowed_start_transitions = torch.zeros(num_tags)

        num_tags = num_tags + 2
        new_transition_matrix = torch.zeros(num_tags, num_tags)
        new_transition_matrix[:-2, :-2] = transition_matrix

        # Start and end transitions are fully defined, but cannot transition between each other.

        allowed_start_transitions = torch.cat(
            [allowed_start_transitions, torch.tensor([-math.inf, -math.inf])]
        )
        allowed_end_transitions = torch.cat(
            [allowed_end_transitions, torch.tensor([-math.inf, -math.inf])]
        )

        # First define how we may transition FROM the start and end tags.
        new_transition_matrix[-2, :] = allowed_start_transitions
        # We cannot transition from the end tag to any tag.
        new_transition_matrix[-1, :] = -math.inf

        new_transition_matrix[:, -1] = allowed_end_transitions
        # We cannot transition to the start tag from any tag.
        new_transition_matrix[:, -2] = -math.inf

        transition_matrix = new_transition_matrix

    if tag_observations:
        if len(tag_observations) != sequence_length:
            raise Exception(
                "Observations were provided, but they were not the same length "
                "as the sequence. Found sequence of length: {} and evidence: {}".format(
                    sequence_length, tag_observations
                )
            )
    else:
        tag_observations = [-1 for _ in range(sequence_length)]

    if has_start_end_restrictions:
        tag_observations = [num_tags - 2] + tag_observations + [num_tags - 1]
        zero_sentinel = torch.zeros(1, num_tags)
        extra_tags_sentinel = torch.ones(sequence_length, 2) * -math.inf
        tag_sequence = torch.cat([tag_sequence, extra_tags_sentinel], -1)
        tag_sequence = torch.cat([zero_sentinel, tag_sequence, zero_sentinel], 0)
        sequence_length = tag_sequence.size(0)

    path_scores = []
    path_indices = []

    if tag_observations[0] != -1:
        one_hot = torch.zeros(num_tags)
        one_hot[tag_observations[0]] = 100000.0
        path_scores.append(one_hot.unsqueeze(0))
    else:
        path_scores.append(tag_sequence[0, :].unsqueeze(0))

    # Evaluate the scores for all possible paths.
    for timestep in range(1, sequence_length):
        # Add pairwise potentials to current scores.
        summed_potentials = path_scores[timestep - 1].unsqueeze(2) + transition_matrix
        summed_potentials = summed_potentials.view(-1, num_tags)

        # Best pairwise potential path score from the previous timestep.
        max_k = min(summed_potentials.size()[0], top_k)
        scores, paths = torch.topk(summed_potentials, k=max_k, dim=0)

        # If we have an observation for this timestep, use it
        # instead of the distribution over tags.
        observation = tag_observations[timestep]
        # Warn the user if they have passed
        # invalid/extremely unlikely evidence.
        if tag_observations[timestep - 1] != -1 and observation != -1:
            if transition_matrix[tag_observations[timestep - 1], observation] < -10000:
                warnings.warn(
                    "The pairwise potential between tags you have passed as "
                    "observations is extremely unlikely. Double check your evidence "
                    "or transition potentials!"
                )
        if observation != -1:
            one_hot = torch.zeros(num_tags)
            one_hot[observation] = 100000.0
            path_scores.append(one_hot.unsqueeze(0))
        else:
            path_scores.append(tag_sequence[timestep, :] + scores)
        path_indices.append(paths.squeeze())

    # Construct the most likely sequence backwards.
    path_scores_v = path_scores[-1].view(-1)
    max_k = min(path_scores_v.size()[0], top_k)
    viterbi_scores, best_paths = torch.topk(path_scores_v, k=max_k, dim=0)
    viterbi_paths = []
    for i in range(max_k):
        viterbi_path = [best_paths[i]]
        for backward_timestep in reversed(path_indices):
            viterbi_path.append(int(backward_timestep.view(-1)[viterbi_path[-1]]))
        # Reverse the backward path.
        viterbi_path.reverse()

        if has_start_end_restrictions:
            viterbi_path = viterbi_path[1:-1]

        # Viterbi paths uses (num_tags * n_permutations) nodes; therefore, we need to modulo.
        viterbi_path = [j % num_tags for j in viterbi_path]
        viterbi_paths.append(viterbi_path)

    if flatten_output:
        return viterbi_paths[0], viterbi_scores[0]

    return viterbi_paths, viterbi_scores


class ConditionalRandomField(torch.nn.Module):
    """
    This module uses the "forward-backward" algorithm to compute
    the log-likelihood of its inputs assuming a conditional random field model.

    See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf

    # Parameters

    num_tags : `int`, required
        The number of tags.
    constraints : `List[Tuple[int, int]]`, optional (default = `None`)
        An optional list of allowed transitions (from_tag_id, to_tag_id).
        These are applied to `viterbi_tags()` but do not affect `forward()`.
        These should be derived from `allowed_transitions` so that the
        start and end transitions are handled correctly for your tag type.
    include_start_end_transitions : `bool`, optional (default = `True`)
        Whether to include the start and end transition parameters.
    """

    def __init__(
            self,
            num_tags: int,
            constraints: List[Tuple[int, int]] = None,
            include_start_end_transitions: bool = True,
    ) -> None:
        super().__init__()
        self.num_tags = num_tags

        # transitions[i, j] is the logit for transitioning from state i to state j.
        self.transitions = torch.nn.Parameter(torch.Tensor(num_tags, num_tags))

        # _constraint_mask indicates valid transitions (based on supplied constraints).
        # Include special start of sequence (num_tags + 1) and end of sequence tags (num_tags + 2)
        if constraints is None:
            # All transitions are valid.
            constraint_mask = torch.Tensor(num_tags + 2, num_tags + 2).fill_(1.0)
        else:
            constraint_mask = torch.Tensor(num_tags + 2, num_tags + 2).fill_(0.0)
            for i, j in constraints:
                constraint_mask[i, j] = 1.0

        self._constraint_mask = torch.nn.Parameter(constraint_mask, requires_grad=False)

        # Also need logits for transitioning from "start" state and to "end" state.
        self.include_start_end_transitions = include_start_end_transitions
        if include_start_end_transitions:
            self.start_transitions = torch.nn.Parameter(torch.Tensor(num_tags))
            self.end_transitions = torch.nn.Parameter(torch.Tensor(num_tags))

        self.reset_parameters()

    def reset_parameters(self):
        torch.nn.init.xavier_normal_(self.transitions)
        if self.include_start_end_transitions:
            torch.nn.init.normal_(self.start_transitions)
            torch.nn.init.normal_(self.end_transitions)

    def _input_likelihood(self, logits: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor:
        """
        Computes the (batch_size,) denominator term for the log-likelihood, which is the
        sum of the likelihoods across all possible state sequences.
        """
        batch_size, sequence_length, num_tags = logits.size()

        # Transpose batch size and sequence dimensions
        mask = mask.transpose(0, 1).contiguous()
        logits = logits.transpose(0, 1).contiguous()

        # Initial alpha is the (batch_size, num_tags) tensor of likelihoods combining the
        # transitions to the initial states and the logits for the first timestep.
        if self.include_start_end_transitions:
            alpha = self.start_transitions.view(1, num_tags) + logits[0]
        else:
            alpha = logits[0]

        # For each i we compute logits for the transitions from timestep i-1 to timestep i.
        # We do so in a (batch_size, num_tags, num_tags) tensor where the axes are
        # (instance, current_tag, next_tag)
        for i in range(1, sequence_length):
            # The emit scores are for time i ("next_tag") so we broadcast along the current_tag axis.
            emit_scores = logits[i].view(batch_size, 1, num_tags)
            # Transition scores are (current_tag, next_tag) so we broadcast along the instance axis.
            transition_scores = self.transitions.view(1, num_tags, num_tags)
            # Alpha is for the current_tag, so we broadcast along the next_tag axis.
            broadcast_alpha = alpha.view(batch_size, num_tags, 1)

            # Add all the scores together and logexp over the current_tag axis.
            inner = broadcast_alpha + emit_scores + transition_scores

            # In valid positions (mask == True) we want to take the logsumexp over the current_tag dimension
            # of `inner`. Otherwise (mask == False) we want to retain the previous alpha.
            alpha = logsumexp(inner, 1) * mask[i].view(batch_size, 1) + alpha * (
                ~mask[i]
            ).view(batch_size, 1)

        # Every sequence needs to end with a transition to the stop_tag.
        if self.include_start_end_transitions:
            stops = alpha + self.end_transitions.view(1, num_tags)
        else:
            stops = alpha

        # Finally we log_sum_exp along the num_tags dim, result is (batch_size,)
        return logsumexp(stops)

    def _joint_likelihood(
            self, logits: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor
    ) -> torch.Tensor:
        """
        Computes the numerator term for the log-likelihood, which is just score(inputs, tags)
        """
        batch_size, sequence_length, _ = logits.data.shape

        # Transpose batch size and sequence dimensions:
        logits = logits.transpose(0, 1).contiguous()
        mask = mask.transpose(0, 1).contiguous()
        tags = tags.transpose(0, 1).contiguous()

        # Start with the transition scores from start_tag to the first tag in each input
        if self.include_start_end_transitions:
            score = self.start_transitions.index_select(0, tags[0])
        else:
            score = 0.0

        # Add up the scores for the observed transitions and all the inputs but the last
        for i in range(sequence_length - 1):
            # Each is shape (batch_size,)
            current_tag, next_tag = tags[i], tags[i + 1]

            # The scores for transitioning from current_tag to next_tag
            transition_score = self.transitions[current_tag.view(-1), next_tag.view(-1)]

            # The score for using current_tag
            emit_score = logits[i].gather(1, current_tag.view(batch_size, 1)).squeeze(1)

            # Include transition score if next element is unmasked,
            # input_score if this element is unmasked.
            score = score + transition_score * mask[i + 1] + emit_score * mask[i]

        # Transition from last state to "stop" state. To start with, we need to find the last tag
        # for each instance.
        last_tag_index = mask.sum(0).long() - 1
        last_tags = tags.gather(0, last_tag_index.view(1, batch_size)).squeeze(0)

        # Compute score of transitioning to `stop_tag` from each "last tag".
        if self.include_start_end_transitions:
            last_transition_score = self.end_transitions.index_select(0, last_tags)
        else:
            last_transition_score = 0.0

        # Add the last input if it's not masked.
        last_inputs = logits[-1]  # (batch_size, num_tags)
        last_input_score = last_inputs.gather(1, last_tags.view(-1, 1))  # (batch_size, 1)
        last_input_score = last_input_score.squeeze()  # (batch_size,)

        score = score + last_transition_score + last_input_score * mask[-1]

        return score

    def forward(
            self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor = None
    ) -> torch.Tensor:
        """
        Computes the log likelihood.
        """

        if mask is None:
            mask = torch.ones_like(tags).type(torch.bool)
            # mask = torch.ones(*tags.size(), dtype=torch.bool)
        else:
            # The code below fails in weird ways if this isn't a bool tensor, so we make sure.
            mask = mask.to(torch.bool)

        log_denominator = self._input_likelihood(inputs, mask)
        log_numerator = self._joint_likelihood(inputs, tags, mask)

        return torch.sum(log_numerator - log_denominator)

    def viterbi_tags(
            self, logits: torch.Tensor, mask: torch.BoolTensor = None, top_k: int = None
    ) -> Union[List[VITERBI_DECODING], List[List[VITERBI_DECODING]]]:
        """
        Uses viterbi algorithm to find most likely tags for the given inputs.
        If constraints are applied, disallows all other transitions.

        Returns a list of results, of the same size as the batch (one result per batch member)
        Each result is a List of length top_k, containing the top K viterbi decodings
        Each decoding is a tuple  (tag_sequence, viterbi_score)

        For backwards compatibility, if top_k is None, then instead returns a flat list of
        tag sequences (the top tag sequence for each batch item).
        """
        if mask is None:
            mask = torch.ones(*logits.shape[:2], dtype=torch.bool, device=logits.device)

        if top_k is None:
            top_k = 1
            flatten_output = True
        else:
            flatten_output = False

        _, max_seq_length, num_tags = logits.size()

        # Get the tensors out of the variables
        logits, mask = logits.data, mask.data

        # Augment transitions matrix with start and end transitions
        start_tag = num_tags
        end_tag = num_tags + 1
        transitions = torch.Tensor(num_tags + 2, num_tags + 2).fill_(-10000.0)

        # Apply transition constraints
        constrained_transitions = self.transitions * self._constraint_mask[
                                                     :num_tags, :num_tags
                                                     ] + -10000.0 * (1 - self._constraint_mask[:num_tags, :num_tags])
        transitions[:num_tags, :num_tags] = constrained_transitions.data

        if self.include_start_end_transitions:
            transitions[
            start_tag, :num_tags
            ] = self.start_transitions.detach() * self._constraint_mask[
                                                  start_tag, :num_tags
                                                  ].data + -10000.0 * (
                        1 - self._constraint_mask[start_tag, :num_tags].detach()
                )
            transitions[:num_tags, end_tag] = self.end_transitions.detach() * self._constraint_mask[
                                                                              :num_tags, end_tag
                                                                              ].data + -10000.0 * (
                                                      1 - self._constraint_mask[:num_tags, end_tag].detach())
        else:
            transitions[start_tag, :num_tags] = -10000.0 * (
                    1 - self._constraint_mask[start_tag, :num_tags].detach()
            )
            transitions[:num_tags, end_tag] = -10000.0 * (
                    1 - self._constraint_mask[:num_tags, end_tag].detach()
            )

        best_paths = []
        # Pad the max sequence length by 2 to account for start_tag + end_tag.
        tag_sequence = torch.Tensor(max_seq_length + 2, num_tags + 2)

        for prediction, prediction_mask in zip(logits, mask):
            mask_indices = prediction_mask.nonzero(as_tuple=False).squeeze()
            masked_prediction = torch.index_select(prediction, 0, mask_indices)
            sequence_length = masked_prediction.shape[0]

            # Start with everything totally unlikely
            tag_sequence.fill_(-10000.0)
            # At timestep 0 we must have the START_TAG
            tag_sequence[0, start_tag] = 0.0
            # At steps 1, ..., sequence_length we just use the incoming prediction
            tag_sequence[1: (sequence_length + 1), :num_tags] = masked_prediction
            # And at the last timestep we must have the END_TAG
            tag_sequence[sequence_length + 1, end_tag] = 0.0

            # We pass the tags and the transitions to `viterbi_decode`.
            viterbi_paths, viterbi_scores = viterbi_decode(
                tag_sequence=tag_sequence[: (sequence_length + 2)],
                transition_matrix=transitions,
                top_k=top_k,
            )
            top_k_paths = []
            for viterbi_path, viterbi_score in zip(viterbi_paths, viterbi_scores):
                # Get rid of START and END sentinels and append.
                viterbi_path = viterbi_path[1:-1]
                top_k_paths.append((viterbi_path, viterbi_score.item()))
            best_paths.append(top_k_paths)

        if flatten_output:
            return [top_k_paths[0] for top_k_paths in best_paths]

        return best_paths


'''
====================================================================================
  @wyzypa End.
====================================================================================
'''

class QETagClassificationPredictionOutput(NamedTuple):
    source_tag_predictions: np.ndarray
    mt_word_tag_predictions: np.ndarray
    mt_gap_tag_predictions: np.ndarray
    source_tag_mask: torch.Tensor
    mt_word_tag_mask: torch.Tensor
    mt_gap_tag_mask: torch.Tensor
    label_ids: Optional[np.ndarray]
    metrics: Optional[Dict[str, float]]


class QETagClassificationTrainer(Trainer):
    '''
        Since my BertForQETagClassification outputs source and MT logits separately, the trainer must be
        re-implemented to adapt that change since the original trainer could only handle one logit output
        In particular, prediction_step and prediction_loop method needs to be overloaded.
    '''

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def prediction_loop(
            self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None
    ) -> QETagClassificationPredictionOutput:

        prediction_loss_only = (
            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only
        )

        model = self.model
        # multi-gpu eval
        if self.args.n_gpu > 1:
            model = torch.nn.DataParallel(model)
        else:
            model = self.model

        batch_size = dataloader.batch_size
        logger.info("***** Running %s *****", description)
        logger.info("  Num examples = %d", self.num_examples(dataloader))
        logger.info("  Batch size = %d", batch_size)
        eval_losses: List[float] = []
        source_tag_preds, source_tag_masks = [], []
        mt_word_tag_preds, mt_word_tag_masks = [], []
        mt_gap_tag_preds, mt_gap_tag_masks = [], []
        label_ids = []
        model.eval()

        if self.args.past_index >= 0:
            self._past = None

        disable_tqdm = not self.is_local_process_zero() or self.args.disable_tqdm
        samples_count = 0
        for inputs in tqdm(dataloader, desc=description, disable=disable_tqdm):
            batch_loss, \
            batch_source_tag_logits, batch_mt_word_tag_logits, batch_mt_gap_tag_logits,\
            batch_source_tag_masks, batch_mt_word_tag_masks, batch_mt_gap_tag_masks, \
            batch_labels = self.prediction_step(model, inputs, prediction_loss_only)

            batch_size = inputs[list(inputs.keys())[0]].shape[0]
            samples_count += batch_size

            if batch_loss is not None:
                eval_losses.append(batch_loss * batch_size)
            source_tag_preds.append(batch_source_tag_logits)
            source_tag_masks.append(batch_source_tag_masks)
            mt_word_tag_preds.append(batch_mt_word_tag_logits)
            mt_word_tag_masks.append(batch_mt_word_tag_masks)
            mt_gap_tag_preds.append(batch_mt_gap_tag_logits)
            mt_gap_tag_masks.append(batch_mt_gap_tag_masks)
            if batch_labels is not None:
                label_ids.append(batch_labels)

        source_tag_preds = torch.cat(source_tag_preds, dim=0)
        source_tag_masks = torch.cat(source_tag_masks, dim=0)
        mt_word_tag_preds = torch.cat(mt_word_tag_preds, dim=0)
        mt_word_tag_masks = torch.cat(mt_word_tag_masks, dim=0)
        mt_gap_tag_preds = torch.cat(mt_gap_tag_preds, dim=0)
        mt_gap_tag_masks = torch.cat(mt_gap_tag_masks, dim=0)

        if len(label_ids) > 0:
            label_ids = torch.cat(label_ids, dim=0)
        else:
            label_ids = None

        if self.args.past_index and hasattr(self, "_past"):
            # Clean the state at the end of the evaluation loop
            delattr(self, "_past")

        if self.args.local_rank != -1:
            # In distributed mode, concatenate all results from all nodes:
            if source_tag_preds is not None:
                source_tag_preds = self.distributed_concat(source_tag_preds,
                                                           num_total_examples=self.num_examples(dataloader))
            if mt_word_tag_preds is not None:
                mt_word_tag_preds = self.distributed_concat(mt_word_tag_preds,
                                                       num_total_examples=self.num_examples(dataloader))
            if mt_gap_tag_preds is not None:
                mt_gap_tag_preds = self.distrbuted_concat(mt_gap_tag_preds,
                                                          num_total_examples=self.num_examples(dataloader))
            if label_ids is not None:
                label_ids = self.distributed_concat(label_ids, num_total_examples=self.num_examples(dataloader))

        # Finally, turn the aggregated tensors into numpy arrays.
        if source_tag_preds is not None:
            source_tag_preds = source_tag_preds.cpu().numpy()
        if mt_word_tag_preds is not None:
            mt_word_tag_preds = mt_word_tag_preds.cpu().numpy()
        if mt_gap_tag_preds is not None:
            mt_gap_tag_preds = mt_gap_tag_preds.cpu().numpy()
        if label_ids is not None:
            label_ids = label_ids.cpu().numpy()

        metrics = {}
        if len(eval_losses) > 0:
            metrics["eval_loss"] = np.sum(eval_losses) / samples_count

        # Prefix all keys with eval_
        for key in list(metrics.keys()):
            if not key.startswith("eval_"):
                metrics[f"eval_{key}"] = metrics.pop(key)

        return QETagClassificationPredictionOutput(source_tag_predictions=source_tag_preds,
                                                   mt_word_tag_predictions=mt_word_tag_preds,
                                                   mt_gap_tag_predictions=mt_gap_tag_preds,
                                                   source_tag_mask=source_tag_masks,
                                                   mt_word_tag_mask=mt_word_tag_masks,
                                                   mt_gap_tag_mask=mt_gap_tag_masks,
                                                   label_ids=label_ids,
                                                   metrics=metrics)

    def prediction_step(
            self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool
    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor],
               Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        has_labels = any(inputs.get(k) is not None for k in ["labels", "lm_labels", "masked_lm_labels", 'word_tag_labels'])

        inputs = self._prepare_inputs(inputs)

        with torch.no_grad():
            outputs = model(**inputs)
            source_tag_mask, mt_word_tag_mask, mt_gap_tag_mask = generate_source_and_mt_tag_mask(inputs['token_type_ids'])
            if has_labels:
                loss, logits = outputs[:2]
                source_tag_logits, mt_word_tag_logits, mt_gap_tag_logits = logits
                loss = loss.mean().item()
            else:
                loss = None
                logits = outputs[0]
                source_tag_logits, mt_word_tag_logits, mt_gap_tag_logits = logits
            if self.args.past_index >= 0:
                self._past = outputs[self.args.past_index if has_labels else self.args.past_index - 1]

        if prediction_loss_only:
            return (loss, None, None, None, None, None, None, None)

        labels = inputs.get("labels")
        if labels is not None:
            labels = labels.detach()
        return (loss, source_tag_logits, mt_word_tag_logits, mt_gap_tag_logits, source_tag_mask, mt_word_tag_mask,
                mt_gap_tag_mask, labels)

    def create_optimizer_and_scheduler(self, num_training_steps: int):
        '''
        copied and modified from Trainer.create_optimizer_and_scheduler
        '''
        if self.optimizer is None:

            no_decay = ["bias", "LayerNorm.weight"]

            if hasattr(self.model, 'source_qe_tag_crf'):
                bert_and_classifier_parameters = list(self.model.bert.named_parameters()) + \
                                                 list(self.model.source_tag_outputs.named_parameters()) + \
                                                 list(self.model.mt_word_tag_outputs.named_parameters())

                crf_parameters = list(self.model.source_qe_tag_crf.named_parameters()) + \
                                 list(self.model.mt_qe_tag_crf.named_parameters())

                optimizer_grouped_parameters = [
                    {
                        "params": [p for n, p in bert_and_classifier_parameters if not any(nd in n for nd in no_decay)],
                        "weight_decay": self.args.weight_decay,
                        "lr": self.args.learning_rate
                    },
                    {
                        "params": [p for n, p in bert_and_classifier_parameters if any(nd in n for nd in no_decay)],
                        "weight_decay": 0.0,
                        "lr": self.args.learning_rate
                    },

                    {
                        "params": [p for n, p in crf_parameters if not any(nd in n for nd in no_decay)],
                        "weight_decay": self.args.weight_decay,
                        "lr": self.args.crf_learning_rate
                    },
                    {
                        "params": [p for n, p in crf_parameters if any(nd in n for nd in no_decay)],
                        "weight_decay": 0.0,
                        "lr": self.args.crf_learning_rate
                    },
                ]
            else:
                optimizer_grouped_parameters = [
                    {
                        "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
                        "weight_decay": self.args.weight_decay,
                    },
                    {
                        "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
                        "weight_decay": 0.0,
                    },
                ]
            self.optimizer = AdamW(
                optimizer_grouped_parameters,
                lr=self.args.learning_rate,
                betas=(self.args.adam_beta1, self.args.adam_beta2),
                eps=self.args.adam_epsilon,
            )
        if self.lr_scheduler is None:
            self.lr_scheduler = get_linear_schedule_with_warmup(
                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps
            )


class BertModelWithQETag(BertPreTrainedModel):
    '''
    This class is temporarily identical with BertModel
    Considering the possible extension in the future, I re-implement this class here.
    '''

    def __init__(self, config):
        super().__init__(config)
        self.config = config

        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)

        self.init_weights()

    def forward(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            encoder_hidden_states=None,
            encoder_attention_mask=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
    ):
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
        elif input_ids is not None:
            input_shape = input_ids.size()
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        device = input_ids.device if input_ids is not None else inputs_embeds.device

        if attention_mask is None:
            attention_mask = torch.ones(input_shape, device=device)
        if token_type_ids is None:
            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)

        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)

        if self.config.is_decoder and encoder_hidden_states is not None:
            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
            if encoder_attention_mask is None:
                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)
        else:
            encoder_extended_attention_mask = None

        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)

        embedding_output = self.embeddings(
            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
        )

        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        sequence_output = encoder_outputs[0]
        pooled_output = self.pooler(sequence_output)

        if not return_dict:
            return (sequence_output, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )


class BertForQETagClassification(BertPreTrainedModel):
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)

        self.bert = BertModelWithQETag(config)

        self.num_label = len(config.label2id)
        self.label2id = config.label2id

        num_label = 1 if self.num_label <= 2 else self.num_label
        self.source_tag_outputs = nn.Sequential(
            nn.Linear(config.hidden_size, num_label),
            nn.Sigmoid()
        )
        self.mt_word_tag_outputs = nn.Sequential(
            nn.Linear(config.hidden_size, num_label),
            nn.Sigmoid()
        )
        self.mt_gap_tag_outputs = nn.Sequential(
            nn.Linear(config.hidden_size, num_label),
            nn.Sigmoid()
        )

        self.bad_loss_lambda = config.bad_loss_lambda

    def forward(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            word_tag_labels=None,
            gap_tag_labels=None,
    ):
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds
        )

        sequence_output = outputs[0]
        source_tag_masks, mt_word_tag_masks, mt_gap_tag_masks = generate_source_and_mt_tag_mask(token_type_ids)
        total_loss = None
        source_tag_logits = self.source_tag_outputs(sequence_output)
        mt_word_tag_logits = self.mt_word_tag_outputs(sequence_output)
        mt_gap_tag_logits = self.mt_gap_tag_outputs(sequence_output)

        if word_tag_labels is not None:
            bce = BCELoss(reduction='sum')
            source_active_logits = source_tag_logits.squeeze(-1).masked_fill(~source_tag_masks, 0.0).view(-1)
            source_active_labels = word_tag_labels.masked_fill(~source_tag_masks, 0).type(torch.float).view(-1)
            if self.bad_loss_lambda != 1.0:
                source_bad_weight = torch.ones_like(source_active_labels).\
                    masked_fill_(source_active_labels.type(torch.bool), self.bad_loss_lambda)
                bce = BCELoss(reduction='sum', weight=source_bad_weight)
            source_tag_loss = bce(source_active_logits, source_active_labels)

            mt_word_active_logits = mt_word_tag_logits.squeeze(-1).masked_fill(~mt_word_tag_masks, 0.0).view(-1)
            mt_word_active_labels = word_tag_labels.masked_fill(~mt_word_tag_masks, 0).type(torch.float).view(-1)
            if self.bad_loss_lambda != 1.0:
                mt_word_bad_weight = torch.ones_like(mt_word_active_labels).\
                    masked_fill_(mt_word_active_labels.type(torch.bool), self.bad_loss_lambda)
                bce = BCELoss(reduction='sum', weight=mt_word_bad_weight)
            mt_word_tag_loss = bce(mt_word_active_logits, mt_word_active_labels)

            mt_gap_tag_loss = None
            if gap_tag_labels is not None:
                mt_gap_active_logits = mt_gap_tag_logits.squeeze(-1).masked_fill(~mt_gap_tag_masks, 0.0).view(-1)
                mt_gap_active_labels = gap_tag_labels.masked_fill(~mt_gap_tag_masks, 0).type(torch.float).view(-1)
                if self.bad_loss_lambda != 1.0:
                    mt_gap_bad_weight = torch.ones_like(mt_gap_active_labels).\
                        masked_fill_(mt_gap_active_labels.type(torch.bool), self.bad_loss_lambda)
                    bce = BCELoss(reduction='sum', weight=mt_gap_bad_weight)
                mt_gap_tag_loss = bce(mt_gap_active_logits, mt_gap_active_labels)

            # mean the loss
            source_tag_loss /= source_tag_masks.sum()
            mt_word_tag_loss /= mt_word_tag_masks.sum()
            if mt_gap_tag_loss is not None:
                mt_gap_tag_loss /= mt_gap_tag_masks.sum()

            total_loss = source_tag_loss + mt_word_tag_loss
            if mt_gap_tag_loss is not None:
                total_loss += mt_gap_tag_loss

        output = ((source_tag_logits, mt_word_tag_logits, mt_gap_tag_logits),) + outputs[2:]
        return ((total_loss,) + output) if total_loss is not None else output

#############################################################
#
#  End
#
#############################################################

@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """
    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    use_fast: bool = field(default=False, metadata={"help": "Set this flag to use fast tokenization."})

    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )

    '''
    ========================================================================================
      @wyzypa
      20210514 add bad_loss_lambda
    ========================================================================================
    '''
    bad_loss_lambda: float = field(
        default=1.0,
        metadata={"help": "A lambda factor justifying loss where tag is BAD."}
    )
    '''
    ========================================================================================
      @wyzypa End.
    ========================================================================================
    '''


@dataclass
class DataTrainingArguments:
    source_text: str = field(
        metadata={'help': 'Path to the source text file.'}
    )
    mt_text: str = field(
        metadata={'help': 'Path to the MT text file.'}
    )
    source_tags: str = field(
        default=None,
        metadata={'help': 'Path to the source QE tags file.'}
    )
    mt_word_tags: str = field(
        default=None,
        metadata={'help': 'Path to the MT QE tags file.'}
    )
    mt_gap_tags: str = field(
        default=None,
        metadata={'help': 'Path to the MT Gap tags file.'}
    )

    max_seq_length: int = field(
        default=128,
        metadata={
            "help": "The maximum total input sequence length after tokenization. Sequences longer "
                    "than this will be truncated, sequences shorter will be padded."
        },
    )
    do_lower_case: bool = field(
        default=False,
        metadata={'help': 'Set this flag if you are using an uncased model.'}
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )

    '''
    ================================================================================
      @wyzypa
      20201104 add tag_prob_threshold
      20201114 add valid_tags
      20210413 add tag_prob_pooling
      20210508 modify tag_prob_threshold to src_prob_threshold & mt_word_prob_threshold & mt_gap_prob_threshold
    ================================================================================
    '''
    source_prob_threshold: float = field(
        default=0.5,
        metadata={"help": "The threshold for predicting source tags in regression mode."}
    )
    mt_word_prob_threshold: float = field(
        default=0.5,
        metadata={"help": "The threshold for predicting source tags in regression mode."}
    )
    mt_gap_prob_threshold: float = field(
        default=0.9,
        metadata={"help": "The threshold for predicting source tags in regression mode."}
    )
    valid_tags: str = field(
        default=None,
        metadata={'help': 'Path to the valid tags file. If not set, tags are expected to be OK/BADs. The most '
                          'frequently used tag is recommended to be placed at first.'}
    )
    tag_prob_pooling: str = field(
        default='max',
        metadata={
            'help': 'Pooling method to merge several token tags into a word tag.',
            'choices': ['mean', 'max', 'min']
        }
    )

    '''
    ================================================================================
      @wyzypa End.
    ================================================================================
    '''


def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))

    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,
    )
    logger.warning(
        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
        training_args.local_rank,
        training_args.device,
        training_args.n_gpu,
        bool(training_args.local_rank != -1),
        training_args.fp16,
    )
    logger.info("Training/evaluation parameters %s", training_args)

    # Set seed
    set_seed(training_args.seed)

    '''
    ==============================================================================
     @ wyzypa
     20201114 read in valid tags file
    ==============================================================================
    '''
    if data_args.valid_tags is not None:
        with open(data_args.valid_tags, 'r') as f:
            labels = f.read().strip().split()
    else:
        labels = ['OK', 'BAD']

    '''
    ==============================================================================
     @wyzypa End.
    ==============================================================================
    '''
    id_to_label: Dict[int, str] = {i: label for i, label in enumerate(labels)}
    label_to_id: Dict[str, int] = {label: i for i, label in enumerate(labels)}
    num_labels = len(labels)

    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=num_labels,
        id2label=id_to_label,
        label2id={label: i for i, label in enumerate(labels)},
        cache_dir=model_args.cache_dir,
    )

    '''
    =================================================================================
      @wyzypa
      20210514 bad_loss_lambda included
    =================================================================================
    '''

    if not hasattr(config, 'bad_loss_lambda'):
        config.bad_loss_lambda = model_args.bad_loss_lambda

    '''
    =================================================================================
      @wyzypa End.
    =================================================================================
    '''

    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        do_lower_case=data_args.do_lower_case,
        cache_dir=model_args.cache_dir,
        use_fast=model_args.use_fast,
    )
    model = BertForQETagClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
    )

    # Get datasets
    train_dataset = (
        QETagClassificationDataset(args=data_args, tokenizer=tokenizer, set_type='train', label_to_id=label_to_id)
        if training_args.do_train
        else None
    )
    eval_dataset = (
        QETagClassificationDataset(args=data_args, tokenizer=tokenizer, set_type='eval', label_to_id=label_to_id)
        if training_args.do_eval
        else None
    )

    def align_predictions(predictions: np.ndarray, mask: torch.Tensor) -> List[List[str]]:
        preds = predictions.squeeze(-1)

        batch_size, max_len = preds.shape[:2]
        res = [[] for _ in range(batch_size)]

        mask = mask.cpu().numpy()
        preds[~mask] = -100

        for i in range(batch_size):
            for j in range(1, max_len):
                if preds[i, j] >= 0:
                    res[i].append(preds[i, j])
                elif preds[i, j - 1] >= 0:
                    break

        return res

    def map_tag_to_origin(line_i, text, tokenizer, tags, pred):

        assert pred in ('source', 'mt_word', 'mt_gap'), f'Invalid predicting flag {pred}.'
        if pred == 'source': threshold = data_args.source_prob_threshold
        elif pred == 'mt_word': threshold = data_args.mt_word_prob_threshold
        else: threshold = data_args.mt_gap_prob_threshold

        pieced_to_origin_map = map_offset(text, tokenizer)
        if pred == 'mt_gap':
            max_i = max(pieced_to_origin_map.keys())
            max_v = max(pieced_to_origin_map.values())
            assert max_i >= len(tags) - 2, f'Inconsistent num of tokens in case:\n{text}\n{tags}'
            if max_i > len(tags) - 2:
                while max_i > len(tags) - 2:
                    tags.append(0.0)
                warnings.warn(f'Line[{line_i}] of [{pred}] seems too long so we have to append more OKs to match the case.')
            pieced_to_origin_map[max_i + 1] = max_v + 1
        else:
            assert max(pieced_to_origin_map.keys()) >= len(tags) - 1, f'Inconsistent num of tokens in case:\n{text}\n{tags}'
            if max(pieced_to_origin_map.keys()) > len(tags) - 1:
                while max(pieced_to_origin_map.keys()) > len(tags) - 1:
                    tags.append(0.0)
                warnings.warn(f'Line[{line_i}] of [{pred}] seems too long so we have to append more OKs to match the case.')
        new_tags = collections.defaultdict(list)
        for i, tag in enumerate(tags):
            new_tags[pieced_to_origin_map[i]].append(tag)

        res = []
        for i in sorted(new_tags):
            vs = new_tags[i]

            if data_args.tag_prob_pooling == 'mean':
                prob = sum(vs) / len(vs)
            elif data_args.tag_prob_pooling == 'max':
                prob = max(vs)
            elif data_args.tag_prob_pooling == 'min':
                prob = min(vs)

            # output tag label text
            res_tag = 1 if prob >= threshold else 0
            res.append(res_tag)

        if pred == 'mt_gap':
            assert len(res) == len(text.split()) + 1
        else:
            assert len(res) == len(text.split())
        return res

    # Initialize our Trainer
    trainer = QETagClassificationTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # Training
    if training_args.do_train:
        trainer.train(
            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
        )
        trainer.save_model()
        if trainer.is_world_master():
            tokenizer.save_pretrained(training_args.output_dir)

    # Predict
    if training_args.do_eval:
        test_dataset = eval_dataset

        source_tag_predictions, mt_word_tag_predictions, mt_gap_tag_predictions,\
        source_tag_mask, mt_word_tag_mask, mt_gap_tag_mask, \
        label_ids, metrics = trainer.predict(test_dataset)

        source_tag_preds = align_predictions(source_tag_predictions, source_tag_mask)
        mt_word_tag_preds = align_predictions(mt_word_tag_predictions, mt_word_tag_mask)
        mt_gap_tag_preds = align_predictions(mt_gap_tag_predictions, mt_gap_tag_mask)

        orig_source_tag_preds = []
        with Path(data_args.source_text).open(encoding='utf-8') as f:
            src_lines = [l.strip() for l in f]
            line_i = 1
            for src_line, source_tag_pred in zip(src_lines, source_tag_preds):
                orig_source_tag_preds.append(map_tag_to_origin(line_i, src_line, tokenizer, source_tag_pred, pred='source'))
                line_i += 1

        orig_mt_word_tag_preds = []
        orig_mt_gap_tag_preds = []
        with Path(data_args.mt_text).open(encoding='utf-8') as f:
            mt_lines = [l.strip() for l in f]
            line_i = 1
            for mt_line, mt_word_tag_pred, mt_gap_tag_pred in \
                    zip(mt_lines, mt_word_tag_preds, mt_gap_tag_preds):
                orig_mt_word_tag_preds.append(map_tag_to_origin(line_i, mt_line, tokenizer, mt_word_tag_pred, pred='mt_word'))
                orig_mt_gap_tag_preds.append(map_tag_to_origin(line_i, mt_line, tokenizer, mt_gap_tag_pred, pred='mt_gap'))
                line_i += 1

        source_tag_res_file = os.path.join(training_args.output_dir, 'pred.source_tags')
        mt_word_tag_res_file = os.path.join(training_args.output_dir, 'pred.mtword_tags')
        mt_gap_tag_res_file = os.path.join(training_args.output_dir, 'pred.gap_tags')

        if trainer.is_world_master():

            with Path(source_tag_res_file).open('w') as f:
                for tags in orig_source_tag_preds:
                    f.write(' '.join(id_to_label[t] for t in tags) + '\n')

            with Path(mt_word_tag_res_file).open('w') as f:
                 for tags in orig_mt_word_tag_preds:
                    f.write(' '.join(id_to_label[t] for t in tags) + '\n')

            with Path(mt_gap_tag_res_file).open('w') as f:
                for tags in orig_mt_gap_tag_preds:
                    f.write(' '.join(id_to_label[t] for t in tags) + '\n')

            with Path(os.path.join(training_args.output_dir, 'gen_config.json')).open('w') as f:
                info = {
                    'time': datetime.datetime.now().strftime('%Y%m%d %H:%M:%S'),
                    'source_prob_threshold': data_args.source_prob_threshold,
                    'mt_word_prob_threshold': data_args.mt_word_prob_threshold,
                    'mt_gap_prob_threshold': data_args.mt_gap_prob_threshold
                }
                for k, v in info.items():
                    f.write(f'{k}: {v}\n')

if __name__ == "__main__":
    main()
