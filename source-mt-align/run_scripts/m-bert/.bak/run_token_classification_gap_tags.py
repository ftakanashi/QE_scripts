# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" Fine-tuning the library models for named entity recognition on CoNLL-2003. """

NOTE = \
'''
    This script is modified from huggingface/transformer's run_ner.py.
    To put it in a simple way, it is a script for token classification, and specifically word-wise word tag prediction.

    
    A typical composition of arguments is like this:
    --model_type bert --model_name_or_path model --do_train --source_text xxx --mt_text xxx --source_word_tags xxx 
    --mt_word_tags xxx --mt_gap_tags xxx --learning_rate 3e-5 --max_seq_length 384 --output_dir output --cache_dir 
    output --save_steps 1000 --num_train_epochs 5.0 --overwrite_cache --overwrite_output_dir --tag_regression
    
    Some newly added arguments are:
    --source_text FILE
    --mt_text FILE
    --source_word_tags FILE
    --mt_word_tags FILE
    --mt_gap_tags FILE    [only required during training]
    --valid_tags FILE
    --tag_regression    [set this flag to transfer classification topping to regression]
    --tag_prob_threshold FLOAT    [only required in testing for regression]
'''


import logging
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

import numpy as np

from transformers import (
    AutoConfig,
    # AutoModelForTokenClassification,
    AutoTokenizer,
    # EvalPrediction,
    HfArgumentParser,
    # Trainer,
    TrainingArguments,
    set_seed,
)

logger = logging.getLogger(__name__)

#############################################################
#
# Start:
# Some of classes and functions about data defined by myself
#
#############################################################

from pathlib import Path
from torch.utils.data import Dataset
from transformers.data import DataProcessor


def map_offset(origin_text, tokenizer):
    '''
    A very lovely method helps to generate an offset mapping dictionary between original tokens of a sentence
    and the tokens generated by BertTokenizer(or other etc.)
    Made special adaption for punctuations like hyphens.
    '''

    orig_tokens = origin_text.split()
    pieced_tokens = []
    for token in tokenizer.basic_tokenizer.tokenize(
            origin_text, never_split=tokenizer.all_special_tokens):
        wp_token = tokenizer.wordpiece_tokenizer.tokenize(token)
        if '[UNK]' in wp_token:    # append the original token rather than UNK to avoid match error
            assert len(wp_token) == 1, f'Token {token} is splited by wordpiece but still contains UNK??'
            pieced_tokens.append(token)
        else:
            pieced_tokens.extend(wp_token)

    mapping = {}
    pieced_i = 0

    for orig_i, orig_token in enumerate(orig_tokens):
        tmp_token = pieced_tokens[pieced_i]

        # normalize orig_token (lowercase, accent-norm. No punc-split-norm)
        if orig_token not in tokenizer.all_special_tokens \
                and orig_token not in tokenizer.basic_tokenizer.never_split:
            # special tokens needs no normalization
            if tokenizer.basic_tokenizer.do_lower_case:
                orig_token = tokenizer.basic_tokenizer._run_strip_accents(orig_token)
                orig_token = orig_token.lower()

        # match!
        while True:
            mapping[pieced_i] = orig_i
            pieced_i += 1
            if tmp_token == orig_token:
                break
            else:
                tmp_token += pieced_tokens[pieced_i].replace('##', '')

            if len(tmp_token) > len(orig_token):    # error raising
                msg = f'Original Text:  {" ".join(orig_tokens)}\n' \
                      f'Pieced Text: {" ".join(pieced_tokens)}\n' \
                      f'Original Token: {orig_token}\n' \
                      f'Pieced Tmp Token: {tmp_token}\n' \
                      f'Mapping: {mapping}'
                raise ValueError('Maybe original tokens and pieced tokens does not match.\n' + msg)

    return mapping


def generate_source_and_mt_tag_mask(token_type_ids):
    '''
    generate masks for source tags and MT tags. Note that CLS and SEP are not included.
    specifically, identifying the min and max indices of 1 in token_type_ids, which respectively identifies the
    first token of MT text and the SEP token of MT text.
    '''
    batch_size, max_len = token_type_ids.shape
    source_tag_masks = []
    mt_tag_masks = []
    for i in range(batch_size):
        row_type_ids = token_type_ids[i, :]
        _mt_indices = row_type_ids.nonzero().view(-1)
        min_i, max_i = _mt_indices.min(), _mt_indices.max()

        source_tag_mask = torch.zeros_like(row_type_ids)
        source_tag_mask[1:min_i - 1] = 1  # CLS and SEP for source text are excluded

        mt_tag_mask = torch.zeros_like(row_type_ids)
        mt_tag_mask[min_i:max_i] = 1  # SEP for MT text is excluded

        source_tag_masks.append(source_tag_mask.type(torch.bool))
        mt_tag_masks.append(mt_tag_mask.type(torch.bool))

    source_tag_masks = torch.stack(source_tag_masks, dim=0)
    mt_tag_masks = torch.stack(mt_tag_masks, dim=0)
    return source_tag_masks, mt_tag_masks

@dataclass
class QETagClassificationInputExample:
    guid: str
    source_text: str
    mt_text: str
    source_word_tags: str
    mt_word_tags: str
    mt_gap_tags: Optional[str] = None

@dataclass
class QETagClassificationInputFeature:
    input_ids: List[int]
    word_tags: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    gap_tag_labels: Optional[List[int]] = None


class QETagClassificationProcessor(DataProcessor):
    def __init__(self, args):
        self.source_text = args.source_text
        self.mt_text = args.mt_text
        self.source_word_tags = args.source_word_tags
        self.mt_word_tags = args.mt_word_tags
        self.mt_gap_tags = args.mt_gap_tags

    def get_examples(self, set_type):

        def read_f(fn):
            with Path(fn).open(encoding='utf-8') as f:
                return [l.strip() for l in f]

        src_lines = read_f(self.source_text)
        mt_lines = read_f(self.mt_text)
        src_wt_lines = read_f(self.source_word_tags)
        mt_wt_lines = read_f(self.mt_word_tags)

        std_len = len(src_lines)
        assert std_len == len(mt_lines) and std_len == len(src_wt_lines) and std_len == len(mt_wt_lines), \
            'Inconsistent number of line'

        if set_type == 'train':
            assert self.mt_gap_tags is not None, 'You need to specify MT gap tags file to do training.'
            mt_gt_lines = read_f(self.mt_gap_tags)
            assert std_len == len(mt_gt_lines), 'Inconsistent number of line'

        elif set_type == 'eval':
            mt_gt_lines = [None] * std_len

        else:
            raise ValueError(f'Invalid set type {set_type}')

        i = 0
        examples = []
        for src_line, mt_line, src_wt_line, mt_wt_line, mt_gt_line in \
                zip(src_lines, mt_lines, src_wt_lines, mt_wt_lines, mt_gt_lines):
            guid = f'{set_type}-{i}'
            examples.append(
                QETagClassificationInputExample(guid=guid, source_text=src_line, mt_text=mt_line,
                                                source_word_tags=src_wt_line, mt_word_tags=mt_wt_line,
                                                mt_gap_tags=mt_gt_line)
            )
            i += 1

        return examples


class QETagClassificationDataset(Dataset):
    def __init__(self, args, tokenizer, set_type, label_to_id):
        self.tokenizer = tokenizer
        self.processor = QETagClassificationProcessor(args)

        msg = f"Creating features from dataset files: {args.source_text}, {args.mt_text}, {args.source_word_tags}, " \
              f"{args.mt_word_tags}"
        if args.mt_gap_tags is not None:
            msg += f', {args.mt_gap_tags}'
        logger.info(msg)

        examples = self.processor.get_examples(set_type)

        batch_text_encoding = tokenizer(
            [(e.source_text, e.mt_text) for e in examples],
            max_length=args.max_seq_length,
            padding="max_length",
            truncation=True,
        )

        qe_tag_map = label_to_id
        id_to_label = {i: label for label, i in label_to_id.items()}
        DEF_TAG = id_to_label[0]
        word_tags_encoding = []
        gap_tags_encoding = []
        for i, e in enumerate(examples):

            # expand original tag sequence into BERT-tokenizer-cut ones
            origin_text = f'{tokenizer.cls_token} {e.source_text} {tokenizer.sep_token} {e.mt_text} ' \
                          f'{tokenizer.sep_token}'
            pieced_to_origin_mapping = map_offset(origin_text, tokenizer)
            max_pieced_token_len = max(pieced_to_origin_mapping.keys()) + 1

            # process word tags
            tag_seq = [DEF_TAG] + e.source_word_tags.split() + [DEF_TAG] + e.mt_word_tags.split() + [DEF_TAG]
            tag_seq = [tag_seq[pieced_to_origin_mapping[k]] for k in range(max_pieced_token_len)]
            while len(tag_seq) < args.max_seq_length:  # padding adaption
                tag_seq.append(DEF_TAG)  # PADs' tag does not really influence for the mask
            if len(tag_seq) > args.max_seq_length:
                # seems source and mt will be truncated respectively to fit the max_seq_length requirement
                # so it is hard to map offset in that case.
                raise ValueError(
                    'I have not done the adaption to qe_tags_input when the text input exceeds max length')

            word_tags_encoding.append([qe_tag_map[t] for t in tag_seq])

            # process gap tags
            if set_type == 'train':
                gap_seq = [DEF_TAG] * (len(e.source_word_tags.split()) + 2) + e.mt_gap_tags.split()
                gap_seq = [gap_seq[pieced_to_origin_mapping[k]] for k in range(max_pieced_token_len)]
                while len(gap_seq) < args.max_seq_length:  # padding adaption
                    gap_seq.append(DEF_TAG)  # PADs' tag does not really influence for the mask
                if len(gap_seq) > args.max_seq_length:
                    raise ValueError(
                        'I have not done the adaption to qe_tags_input when the text input exceeds max length')
                gap_tags_encoding.append([qe_tag_map[t] for t in gap_seq])
            else:
                gap_tags_encoding.append(None)

        self.features = []
        for i in range(len(examples)):
            text_inputs = {k: batch_text_encoding[k][i] for k in batch_text_encoding}
            word_tags = word_tags_encoding[i]
            gap_tags = gap_tags_encoding[i]
            feature = QETagClassificationInputFeature(input_ids=text_inputs['input_ids'],
                                                      attention_mask=text_inputs['attention_mask'],
                                                      token_type_ids=text_inputs['token_type_ids'],
                                                      word_tags=word_tags,
                                                      gap_tag_labels=gap_tags)
            self.features.append(feature)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i) -> QETagClassificationInputFeature:
        return self.features[i]


#############################################################
#
#  End
#
#############################################################


#############################################################
#
# Start:
# Some of classes and functions about model defined by myself
#
#############################################################
import collections
import math
import torch
import warnings
from torch import nn
from torch.utils.data import DataLoader
from torch.nn.modules.loss import CrossEntropyLoss, BCELoss
from transformers.modeling_bert import BertModel, BertPreTrainedModel, BertEmbeddings, BertEncoder, BertPooler
from transformers.modeling_outputs import BaseModelOutputWithPooling
from transformers.trainer import Trainer
from transformers.optimization import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm
from typing import Union, Any, NamedTuple, List, Tuple, Optional

class QETagClassificationPredictionOutput(NamedTuple):
    gap_tag_predictions: np.ndarray
    gap_tag_mask: torch.Tensor
    label_ids: Optional[np.ndarray]
    metrics: Optional[Dict[str, float]]


class QETagClassificationTrainer(Trainer):
    '''
        Since my BertForQETagClassification outputs source and MT logits separately, the trainer must be
        re-implemented to adapt that change since the original trainer could only handle one logit output
        In particular, prediction_step and prediction_loop method needs to be overloaded.
    '''

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def prediction_loop(
            self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None
    ) -> QETagClassificationPredictionOutput:

        prediction_loss_only = (
            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only
        )

        model = self.model
        # multi-gpu eval
        if self.args.n_gpu > 1:
            model = torch.nn.DataParallel(model)
        else:
            model = self.model

        batch_size = dataloader.batch_size
        logger.info("***** Running %s *****", description)
        logger.info("  Num examples = %d", self.num_examples(dataloader))
        logger.info("  Batch size = %d", batch_size)
        eval_losses: List[float] = []
        gap_tag_preds = []
        gap_tag_masks = []
        label_ids = []
        model.eval()

        if self.args.past_index >= 0:
            self._past = None

        disable_tqdm = not self.is_local_process_zero() or self.args.disable_tqdm
        samples_count = 0
        for inputs in tqdm(dataloader, desc=description, disable=disable_tqdm):
            batch_loss, batch_gap_tag_logits, batch_gap_tag_masks, batch_labels = \
                self.prediction_step(model, inputs, prediction_loss_only)

            batch_size = inputs[list(inputs.keys())[0]].shape[0]
            samples_count += batch_size

            if batch_loss is not None:
                eval_losses.append(batch_loss * batch_size)
            gap_tag_preds.append(batch_gap_tag_logits)
            gap_tag_masks.append(batch_gap_tag_masks)
            if batch_labels is not None:
                label_ids.append(batch_labels)

        gap_tag_preds = torch.cat(gap_tag_preds, dim=0)
        gap_tag_masks = torch.cat(gap_tag_masks, dim=0)
        if len(label_ids) > 0:
            label_ids = torch.cat(label_ids, dim=0)
        else:
            label_ids = None

        if self.args.past_index and hasattr(self, "_past"):
            # Clean the state at the end of the evaluation loop
            delattr(self, "_past")

        if self.args.local_rank != -1:
            # In distributed mode, concatenate all results from all nodes:
            if gap_tag_preds is not None:
                gap_tag_preds = self.distributed_concat(gap_tag_preds,
                                                        num_total_examples=self.num_examples(dataloader))
            if label_ids is not None:
                label_ids = self.distributed_concat(label_ids, num_total_examples=self.num_examples(dataloader))

        # Finally, turn the aggregated tensors into numpy arrays.
        if gap_tag_preds is not None:
            gap_tag_preds = gap_tag_preds.cpu().numpy()
        if label_ids is not None:
            label_ids = label_ids.cpu().numpy()

        metrics = {}
        if len(eval_losses) > 0:
            metrics["eval_loss"] = np.sum(eval_losses) / samples_count

        # Prefix all keys with eval_
        for key in list(metrics.keys()):
            if not key.startswith("eval_"):
                metrics[f"eval_{key}"] = metrics.pop(key)

        return QETagClassificationPredictionOutput(gap_tag_predictions=gap_tag_preds,
                                                   gap_tag_mask=gap_tag_masks,
                                                   label_ids=label_ids,
                                                   metrics=metrics)

    def prediction_step(
            self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool
    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
        has_labels = any(inputs.get(k) is not None for k in ["labels", "lm_labels", "masked_lm_labels",
                                                             'gap_tag_labels'])

        inputs = self._prepare_inputs(inputs)

        with torch.no_grad():
            outputs = model(**inputs)
            gap_tag_mask = inputs['token_type_ids'].type(torch.bool)
            if has_labels:
                loss, gap_tag_logits = outputs[:2]
                loss = loss.mean().item()
            else:
                loss = None
                gap_tag_logits = outputs[0]
            if self.args.past_index >= 0:
                self._past = outputs[self.args.past_index if has_labels else self.args.past_index - 1]

        if prediction_loss_only:
            return (loss, None, None, None)

        labels = inputs.get("gap_tag_labels")
        if labels is not None:
            labels = labels.detach()
        return (loss, gap_tag_logits, gap_tag_mask, labels)

    def create_optimizer_and_scheduler(self, num_training_steps: int):
        '''
        copied and modified from Trainer.create_optimizer_and_scheduler
        '''
        if self.optimizer is None:

            no_decay = ["bias", "LayerNorm.weight"]

            if hasattr(self.model, 'source_qe_tag_crf'):
                bert_and_classifier_parameters = list(self.model.bert.named_parameters()) + \
                                                 list(self.model.source_qe_tag_outputs.named_parameters()) + \
                                                 list(self.model.mt_qe_tag_outputs.named_parameters())

                crf_parameters = list(self.model.source_qe_tag_crf.named_parameters()) + \
                                 list(self.model.mt_qe_tag_crf.named_parameters())

                optimizer_grouped_parameters = [
                    {
                        "params": [p for n, p in bert_and_classifier_parameters if not any(nd in n for nd in no_decay)],
                        "weight_decay": self.args.weight_decay,
                        "lr": self.args.learning_rate
                    },
                    {
                        "params": [p for n, p in bert_and_classifier_parameters if any(nd in n for nd in no_decay)],
                        "weight_decay": 0.0,
                        "lr": self.args.learning_rate
                    },

                    {
                        "params": [p for n, p in crf_parameters if not any(nd in n for nd in no_decay)],
                        "weight_decay": self.args.weight_decay,
                        "lr": self.args.crf_learning_rate
                    },
                    {
                        "params": [p for n, p in crf_parameters if any(nd in n for nd in no_decay)],
                        "weight_decay": 0.0,
                        "lr": self.args.crf_learning_rate
                    },
                ]
            else:
                optimizer_grouped_parameters = [
                    {
                        "params": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
                        "weight_decay": self.args.weight_decay,
                    },
                    {
                        "params": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],
                        "weight_decay": 0.0,
                    },
                ]
            self.optimizer = AdamW(
                optimizer_grouped_parameters,
                lr=self.args.learning_rate,
                betas=(self.args.adam_beta1, self.args.adam_beta2),
                eps=self.args.adam_epsilon,
            )
        if self.lr_scheduler is None:
            self.lr_scheduler = get_linear_schedule_with_warmup(
                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps
            )


class BertModelWithQETag(BertPreTrainedModel):
    '''
    This class is temporarily identical with BertModel
    Considering the possible extension in the future, I re-implement this class here.
    '''

    def __init__(self, config):
        super().__init__(config)
        self.config = config

        self.embeddings = BertEmbeddings(config)
        self.tag_embedding = nn.Embedding(config.num_labels, config.hidden_size)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)

        self.init_weights()

    def forward(
            self,
            input_ids=None,
            word_tags=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            encoder_hidden_states=None,
            encoder_attention_mask=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
    ):
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
        elif input_ids is not None:
            input_shape = input_ids.size()
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        device = input_ids.device if input_ids is not None else inputs_embeds.device

        if attention_mask is None:
            attention_mask = torch.ones(input_shape, device=device)
        if token_type_ids is None:
            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)

        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)

        if self.config.is_decoder and encoder_hidden_states is not None:
            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
            if encoder_attention_mask is None:
                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)
        else:
            encoder_extended_attention_mask = None

        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)

        embedding_output = self.embeddings(
            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
        )

        tag_embedding_output = self.tag_embedding(word_tags)
        embedding_output = embedding_output + tag_embedding_output

        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        sequence_output = encoder_outputs[0]
        pooled_output = self.pooler(sequence_output)

        if not return_dict:
            return (sequence_output, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )


class BertForQETagClassification(BertPreTrainedModel):
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)

        self.bert = BertModelWithQETag(config)

        self.num_label = len(config.label2id)
        self.label2id = config.label2id
        self.tag_regression = config.tag_regression
        if config.tag_regression:
            num_label = 1 if self.num_label <= 2 else self.num_label
            self.mt_gap_tag_outputs = nn.Sequential(
                nn.Linear(config.hidden_size, num_label),
                nn.Sigmoid()
            )
        else:
            self.mt_gap_tag_outputs = nn.Linear(config.hidden_size, self.num_label)

    def forward(
            self,
            input_ids=None,
            word_tags=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            gap_tag_labels=None,
    ):
        outputs = self.bert(
            input_ids,
            word_tags,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds
        )

        sequence_output = outputs[0]
        gap_tag_masks = token_type_ids.type(torch.bool)
        total_loss = None
        gap_tag_logits = self.mt_gap_tag_outputs(sequence_output)

        if gap_tag_labels is not None:

            # loss for regression
            if self.tag_regression:
                bce = BCELoss(reduction='sum')
                if self.num_label == 2:
                    gap_active_logits = gap_tag_logits.squeeze(-1).masked_fill(~gap_tag_masks, 0.0)
                    gap_tag_loss = bce(gap_active_logits, gap_tag_labels.masked_fill(~gap_tag_masks,
                                                                                     0).type(torch.float))
                else:
                    batch_size, seq_len = gap_tag_masks.shape
                    gap_mask_exp = (~gap_tag_masks).unsqueeze(-1).expand(batch_size, seq_len, self.num_label)
                    gap_active_logits = gap_tag_logits.masked_fill(gap_mask_exp, 0.0)
                    gap_tag_loss = bce(gap_active_logits, gap_tag_labels.masked_fill(gap_mask_exp, 0).type(torch.float))

                # mean the loss
                gap_tag_loss /= gap_tag_masks.sum()

            # loss for classification
            else:
                loss_fct = CrossEntropyLoss()
                gap_active_tag_labels = torch.where(gap_tag_masks.view(-1), gap_tag_labels.view(-1),
                                                    torch.tensor(loss_fct.ignore_index).type_as(gap_tag_labels))
                gap_tag_loss = loss_fct(gap_tag_logits.view(-1, 2), gap_active_tag_labels)

            total_loss = gap_tag_loss

        output = (gap_tag_logits, ) + outputs[2:]
        return ((total_loss,) + output) if total_loss is not None else output


#############################################################
#
#  End
#
#############################################################

@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """
    model_type: str = field(
        metadata={"help": "Type of model"}
    )
    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    # task_type: Optional[str] = field(
    #     default="NER", metadata={"help": "Task type to fine tune in training (e.g. NER, POS, etc)"}
    # )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    use_fast: bool = field(default=False, metadata={"help": "Set this flag to use fast tokenization."})

    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )

    '''
    ========================================================================================
      @wyzypa
      20201213 add new arguments
    ========================================================================================
    '''
    tag_regression: bool = field(
        default=False,
        metadata={"help": "Set this flag to change the classification for top layers to regression (probability "
                          "prediction). Note that only effective in 2-class tag classification."}
    )
    '''
    ========================================================================================
      @wyzypa End.
    ========================================================================================
    '''


@dataclass
class DataTrainingArguments:
    source_text: str = field(
        metadata={'help': 'Path to the source text file.'}
    )
    mt_text: str = field(
        metadata={'help': 'Path to the MT text file.'}
    )
    source_word_tags: str = field(
        metadata={'help': 'Path to the source QE tags file.'}
    )
    mt_word_tags: str = field(
        metadata={'help': 'Path to the MT QE tags file.'}
    )
    mt_gap_tags: str = field(
        default=None,
        metadata={'help': 'Path to the MT gap tags file.'}
    )

    max_seq_length: int = field(
        default=128,
        metadata={
            "help": "The maximum total input sequence length after tokenization. Sequences longer "
                    "than this will be truncated, sequences shorter will be padded."
        },
    )
    do_lower_case: bool = field(
        default=False,
        metadata={'help': 'Set this flag if you are using an uncased model.'}
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )

    '''
    ================================================================================
      @wyzypa
      20201213 add arguments
    ================================================================================
    '''
    tag_prob_threshold: float = field(
        default=0.5,
        metadata={"help": "The threshold for predicting tag in regression mode. Only effective during prediction when --tag_regression is specified."}
    )
    valid_tags: str = field(
        default=None,
        metadata={'help': 'Path to the valid tags file. If not set, tags are expected to be OK/BADs. The most '
                          'frequently used tag is recommended to be placed at first.'}
    )
    '''
    ================================================================================
      @wyzypa End.
    ================================================================================
    '''


def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))

    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,
    )
    logger.warning(
        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
        training_args.local_rank,
        training_args.device,
        training_args.n_gpu,
        bool(training_args.local_rank != -1),
        training_args.fp16,
    )
    logger.info("Training/evaluation parameters %s", training_args)

    # Set seed
    set_seed(training_args.seed)

    '''
    ==============================================================================
     @ wyzypa
     20201114 read in valid tags file
    ==============================================================================
    '''
    if data_args.valid_tags is not None:
        with open(data_args.valid_tags, 'r') as f:
            labels = f.read().strip().split()
    else:
        labels = ['OK', 'BAD']

    assert labels == ['OK', 'BAD'], 'Only OK/BAD supported now.'
    '''
    ==============================================================================
     @wyzypa End.
    ==============================================================================
    '''
    id_to_label: Dict[int, str] = {i: label for i, label in enumerate(labels)}
    label_to_id: Dict[str, int] = {label: i for i, label in enumerate(labels)}
    num_labels = len(labels)

    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=num_labels,
        id2label=id_to_label,
        label2id={label: i for i, label in enumerate(labels)},
        cache_dir=model_args.cache_dir,
    )

    '''
    =================================================================================
      @wyzypa
      20201213 tag_regression included
    =================================================================================
    '''

    if not hasattr(config, 'tag_regression'):
        config.tag_regression = model_args.tag_regression

    '''
    =================================================================================
      @wyzypa End.
    =================================================================================
    '''

    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        do_lower_case=data_args.do_lower_case,
        cache_dir=model_args.cache_dir,
        use_fast=model_args.use_fast,
    )
    model = BertForQETagClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
    )

    # Get datasets
    train_dataset = (
        QETagClassificationDataset(args=data_args, tokenizer=tokenizer, set_type='train', label_to_id=label_to_id)
        if training_args.do_train
        else None
    )
    eval_dataset = (
        QETagClassificationDataset(args=data_args, tokenizer=tokenizer, set_type='eval', label_to_id=label_to_id)
        if training_args.do_eval
        else None
    )

    def align_predictions(predictions: np.ndarray, mask: torch.Tensor) -> List[List[str]]:
        regression_mode = config.tag_regression
        if regression_mode:
            if predictions.shape[-1] > 1:
                # multi-label regression
                preds = predictions
            else:
                preds = predictions.squeeze(-1)
        else:
            # classification
            preds = np.argmax(predictions, axis=2)

        batch_size, max_len = preds.shape[:2]
        res = [[] for _ in range(batch_size)]

        mask = mask.cpu().numpy()
        preds[~mask] = -100

        if preds.ndim == 2:
            for i in range(batch_size):
                for j in range(1, max_len):
                    if preds[i, j] >= 0:
                        res[i].append(preds[i, j])
                    elif preds[i, j - 1] >= 0:
                        break
        else:
            for i in range(batch_size):
                for j in range(1, max_len):
                    if preds[i, j].min() >= 0:
                        res[i].append(preds[i, j])
                    elif preds[i, j - 1].min() >= 0:
                        break

        return res

    def map_tag_to_origin(text, tokenizer, tags):
        pieced_to_origin_map = map_offset(text, tokenizer)
        assert max(pieced_to_origin_map.keys()) == len(tags) - 1, f'Inconsistent num of tokens in case:\n{text}\n{tags}'
        new_tags = collections.defaultdict(list)
        for i, tag in enumerate(tags):
            new_tags[pieced_to_origin_map[i]].append(tag)

        res = []
        if config.tag_regression:
            for i in sorted(new_tags):
                vs = new_tags[i]
                mean_prob = sum(vs) / len(vs)
                if num_labels == 2:
                    # output tag label text
                    res_tag = 1 if mean_prob >= data_args.tag_prob_threshold else 0
                else:
                    res_tag = '|'.join([str(f) for f in list(mean_prob)])
                res.append(res_tag)

        else:
            for i in sorted(new_tags):
                c = collections.Counter(new_tags[i])
                res.append(c.most_common(1)[0][0])

        assert len(res) == len(text.split())
        return res

    # Initialize our Trainer
    trainer = QETagClassificationTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # Training
    if training_args.do_train:
        trainer.train(
            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
        )
        trainer.save_model()
        if trainer.is_world_master():
            tokenizer.save_pretrained(training_args.output_dir)

    # Predict
    if training_args.do_eval:
        test_dataset = eval_dataset

        gap_tag_predictions, gap_tag_mask, label_ids, metrics = trainer.predict(test_dataset)
        gap_tag_preds = align_predictions(gap_tag_predictions, gap_tag_mask)

        orig_gap_tag_preds = []
        with Path(data_args.mt_text).open(encoding='utf-8') as f:
            mt_lines = [l.strip() for l in f]
            for mt_line, gap_tag_pred in zip(mt_lines, gap_tag_preds):
                mt_line = mt_line + f' {tokenizer.sep_token}'
                orig_gap_tag_preds.append(map_tag_to_origin(mt_line, tokenizer, gap_tag_pred))

        if num_labels == 2:
            gap_tag_res_file = os.path.join(training_args.output_dir, 'pred.gap_tags')
        else:
            gap_tag_res_file = os.path.join(training_args.output_dir, 'pred.gap_tags.prob')

        if trainer.is_world_master():
            # id_to_label = {i: t for i, t in enumerate(config.pair_wise_regression.split(','))}

            with Path(gap_tag_res_file).open('w') as f:
                for tags in orig_gap_tag_preds:
                    if num_labels == 2:    # binary regression or classification
                        f.write(' '.join(id_to_label[t] for t in tags) + '\n')
                    else:    # multi-label regression
                        f.write(' '.join(t for t in tags) + '\n')


if __name__ == "__main__":
    main()
