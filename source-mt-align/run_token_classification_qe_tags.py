# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""" Fine-tuning the library models for named entity recognition on CoNLL-2003. """

NOTE = \
'''
    This script is modified from huggingface/transformer's run_ner.py.
    To put it in a simple way, it is a script for token classification, and specifically word-wise word tag prediction.
    Some newly added arguments are:
    --source_text FILE
    --mt_text FILE
    --source_qe_tags FILE    [only required in training]
    --mt_qe_tags FILE    [only required in training]
    
    A typical composition of arguments is like this:
    --model_type bert --model_name_or_path model --do_train --source_text xxx --mt_text xxx --source_qe_tags xxx 
    --mt_qe_tags xxx --max_seq_length 384 --output_dir output --per_device_train_batch_size 8 --save_steps 10000 
    --num_train_epochs 5.0 --overwrite_cache --overwrite_output_dir
'''


import logging
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple

import numpy as np

from transformers import (
    AutoConfig,
    # AutoModelForTokenClassification,
    AutoTokenizer,
    # EvalPrediction,
    HfArgumentParser,
    # Trainer,
    TrainingArguments,
    set_seed,
)

# from utils_ner import Split, TokenClassificationDataset, TokenClassificationTask


logger = logging.getLogger(__name__)

#############################################################
#
# Start:
# Some of classes and functions about data defined by myself
#
#############################################################

from pathlib import Path
from torch.utils.data import Dataset
from transformers.data import DataProcessor


def map_offset(origin_text, tokenizer):
    '''
    A very lovely method helps to generate an offset mapping dictionary between original tokens of a sentence
    and the tokens generated by BertTokenizer(or other etc.)
    Made special adaption for punctuations like hyphens.
    '''

    orig_tokens = origin_text.split()
    pieced_tokens = []
    for token in tokenizer.basic_tokenizer.tokenize(
            origin_text, never_split=tokenizer.all_special_tokens):
        wp_token = tokenizer.wordpiece_tokenizer.tokenize(token)
        if '[UNK]' in wp_token:    # append the original token rather than UNK to avoid match error
            assert len(wp_token) == 1, f'Token {token} is splited by wordpiece but still contains UNK??'
            pieced_tokens.append(token)
        else:
            pieced_tokens.extend(wp_token)

    mapping = {}
    pieced_i = 0

    for orig_i, orig_token in enumerate(orig_tokens):
        tmp_token = pieced_tokens[pieced_i]

        # normalize orig_token (lowercase, accent-norm. No punc-split-norm)
        if orig_token not in tokenizer.all_special_tokens \
                and orig_token not in tokenizer.basic_tokenizer.never_split:
            # special tokens needs no normalization
            if tokenizer.basic_tokenizer.do_lower_case:
                orig_token = tokenizer.basic_tokenizer._run_strip_accents(orig_token)
                orig_token = orig_token.lower()

        # match!
        while True:
            mapping[pieced_i] = orig_i
            pieced_i += 1
            if tmp_token == orig_token:
                break
            else:
                tmp_token += pieced_tokens[pieced_i].replace('##', '')

            if len(tmp_token) > len(orig_token):    # error raising
                msg = f'Original Text:  {" ".join(orig_tokens)}\n' \
                      f'Pieced Text: {" ".join(pieced_tokens)}\n' \
                      f'Original Token: {orig_token}\n' \
                      f'Pieced Tmp Token: {tmp_token}\n' \
                      f'Mapping: {mapping}'
                raise ValueError('Maybe original tokens and pieced tokens does not match.\n' + msg)

    return mapping


@dataclass
class QETagClassificationInputExample:
    guid: str
    source_text: str
    mt_text: str
    source_qe_tags: Optional[str] = None
    mt_qe_tags: Optional[str] = None


@dataclass
class QETagClassificationInputFeature:
    input_ids: List[int]
    attention_mask: Optional[List[int]] = None
    token_type_ids: Optional[List[int]] = None
    tag_labels: Optional[List[int]] = None


class QETagClassificationProcessor(DataProcessor):
    def __init__(self, args):
        self.source_text = args.source_text
        self.mt_text = args.mt_text
        self.source_qe_tags = args.source_qe_tags
        self.mt_qe_tags = args.mt_qe_tags

    def get_examples(self, set_type):

        def read_f(fn):
            with Path(fn).open(encoding='utf-8') as f:
                return [l.strip() for l in f]

        src_lines = read_f(self.source_text)
        mt_lines = read_f(self.mt_text)

        assert len(src_lines) == len(mt_lines), 'Inconsistent number of line'

        if set_type == 'train':
            assert self.source_qe_tags is not None, 'You need to specify source QE Tags file to do train.'
            assert self.mt_qe_tags is not None, 'You need to specify MT QE Tags file to do train.'
            source_qe_tags_lines = read_f(self.source_qe_tags)
            mt_qe_tags_lines = read_f(self.mt_qe_tags)
            assert len(src_lines) == len(source_qe_tags_lines), 'Inconsistent number of line'
            assert len(src_lines) == len(mt_qe_tags_lines), 'Inconsistent number of line'

        elif set_type == 'eval':
            source_qe_tags_lines = [None] * len(src_lines)
            mt_qe_tags_lines = [None] * len(src_lines)

        else:
            raise ValueError(f'Invalid set type {set_type}')

        i = 0
        examples = []
        for src_line, mt_line, source_qe_tags_line, mt_qe_tags_line in zip(src_lines, mt_lines, source_qe_tags_lines,
                                                                           mt_qe_tags_lines):
            guid = f'{set_type}-{i}'
            examples.append(
                QETagClassificationInputExample(guid=guid, source_text=src_line, mt_text=mt_line,
                                                source_qe_tags=source_qe_tags_line, mt_qe_tags=mt_qe_tags_line)
            )
            i += 1

        return examples


class QETagClassificationDataset(Dataset):
    def __init__(self, args, tokenizer, set_type, label_to_id):
        self.tokenizer = tokenizer
        self.processor = QETagClassificationProcessor(args)

        if args.source_qe_tags is not None:
            assert args.mt_qe_tags is not None, 'You must specify MT QE tags simultaneously'
        else:
            assert args.mt_qe_tags is None, 'You must specify Source QE tags simultaneously.'

        msg = f"Creating features from dataset files: {args.source_text}, {args.mt_text}"
        if args.source_qe_tags is not None and args.mt_qe_tags is not None:
            msg += f', {args.source_qe_tags}, {args.mt_qe_tags}'
        logger.info(msg)

        examples = self.processor.get_examples(set_type)

        batch_text_encoding = tokenizer(
            [(e.source_text, e.mt_text) for e in examples],
            max_length=args.max_seq_length,
            padding="max_length",
            truncation=True,
        )

        if set_type == 'train':
            qe_tag_map = label_to_id
            batch_qe_tags_encoding = []
            for i, e in enumerate(examples):
                source_qe_tags = e.source_qe_tags.split()
                mt_qe_tags = e.mt_qe_tags.split()
                if len(mt_qe_tags) == 2 * len(e.mt_text.split()) + 1:
                    mt_qe_tags = mt_qe_tags[1::2]
                # default QE tag for CLS and SEP are OK
                qe_tag_encoding = ['OK'] + source_qe_tags + ['OK'] + mt_qe_tags + ['OK']

                origin_text = f'{tokenizer.cls_token} {e.source_text} {tokenizer.sep_token} {e.mt_text} ' \
                              f'{tokenizer.sep_token}'
                pieced_to_origin_mapping = map_offset(origin_text, tokenizer)
                max_pieced_token_len = max(pieced_to_origin_mapping.keys()) + 1
                pieced_qe_tag_encoding = [qe_tag_encoding[pieced_to_origin_mapping[k]] for k in
                                          range(max_pieced_token_len)]
                qe_tag_encoding = pieced_qe_tag_encoding

                while len(qe_tag_encoding) < args.max_seq_length:  # padding adaption
                    qe_tag_encoding.append('BAD')  # PAD are set to BAD in default

                if len(qe_tag_encoding) > args.max_seq_length:
                    # seems source and mt will be truncated respectively to fit the max_seq_length requirement
                    # so it is hard to map offset in that case.
                    raise ValueError(
                        'I have not done the adaption to qe_tags_input when the text input exceeds max length')

                batch_qe_tags_encoding.append([qe_tag_map[t] for t in qe_tag_encoding])

        else:
            batch_qe_tags_encoding = [None for e in examples]

        self.features = []
        for i in range(len(examples)):
            text_inputs = {k: batch_text_encoding[k][i] for k in batch_text_encoding}
            qe_tags_inputs = batch_qe_tags_encoding[i]
            feature = QETagClassificationInputFeature(input_ids=text_inputs['input_ids'],
                                                      attention_mask=text_inputs['attention_mask'],
                                                      token_type_ids=text_inputs['token_type_ids'],
                                                      tag_labels=qe_tags_inputs)
            self.features.append(feature)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, i) -> QETagClassificationInputFeature:
        return self.features[i]




#############################################################
#
#  End
#
#############################################################


#############################################################
#
# Start:
# Some of classes and functions about model defined by myself
#
#############################################################
import collections
import torch
from torch import nn
from torch.utils.data import DataLoader
from torch.nn.modules.loss import CrossEntropyLoss
from transformers.modeling_bert import BertModel, BertPreTrainedModel, BertEmbeddings, BertEncoder, BertPooler
from transformers.modeling_outputs import BaseModelOutputWithPooling
from transformers.trainer import Trainer
from tqdm import tqdm
from typing import Union, Any, NamedTuple


class QETagClassificationPredictionOutput(NamedTuple):
    source_tag_predictions: np.ndarray
    mt_tag_predictions: np.ndarray
    source_tag_mask: torch.Tensor
    mt_tag_mask: torch.Tensor
    label_ids: Optional[np.ndarray]
    metrics: Optional[Dict[str, float]]


class QETagClassificationTrainer(Trainer):
    '''
        Since my BertForQETagClassification outputs source and MT logits separately, the trainer must be
        re-implemented to adapt that change since the original trainer could only handle one logit output
        In particular, prediction_step and prediction_loop method needs to be overloaded.
    '''

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def prediction_loop(
            self, dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool] = None
    ) -> QETagClassificationPredictionOutput:

        prediction_loss_only = (
            prediction_loss_only if prediction_loss_only is not None else self.args.prediction_loss_only
        )

        model = self.model
        # multi-gpu eval
        if self.args.n_gpu > 1:
            model = torch.nn.DataParallel(model)
        else:
            model = self.model

        batch_size = dataloader.batch_size
        logger.info("***** Running %s *****", description)
        logger.info("  Num examples = %d", self.num_examples(dataloader))
        logger.info("  Batch size = %d", batch_size)
        eval_losses: List[float] = []
        # preds: torch.Tensor = None
        source_tag_preds: torch.Tensor = None
        mt_tag_preds: torch.Tensor = None
        label_ids: torch.Tensor = None
        model.eval()

        if self.args.past_index >= 0:
            self._past = None

        disable_tqdm = not self.is_local_process_zero() or self.args.disable_tqdm
        samples_count = 0
        for inputs in tqdm(dataloader, desc=description, disable=disable_tqdm):
            loss, source_tag_logits, mt_tag_logits, source_tag_mask, mt_tag_mask, labels = \
                self.prediction_step(model, inputs, prediction_loss_only)
            batch_size = inputs[list(inputs.keys())[0]].shape[0]
            samples_count += batch_size
            if loss is not None:
                eval_losses.append(loss * batch_size)
            source_tag_preds = source_tag_logits
            mt_tag_preds = mt_tag_logits
            # if logits is not None:
            #     preds = logits if preds is None else torch.cat((preds, logits), dim=0)
            if labels is not None:
                label_ids = labels if label_ids is None else torch.cat((label_ids, labels), dim=0)

        if self.args.past_index and hasattr(self, "_past"):
            # Clean the state at the end of the evaluation loop
            delattr(self, "_past")

        if self.args.local_rank != -1:
            # In distributed mode, concatenate all results from all nodes:
            if source_tag_preds is not None:
                source_tag_preds = self.distributed_concat(source_tag_preds,
                                                           num_total_examples=self.num_examples(dataloader))
            if mt_tag_preds is not None:
                mt_tag_preds = self.distributed_concat(mt_tag_preds,
                                                       num_total_examples=self.num_examples(dataloader))
            # if preds is not None:
            #     preds = self.distributed_concat(preds, num_total_examples=self.num_examples(dataloader))
            if label_ids is not None:
                label_ids = self.distributed_concat(label_ids, num_total_examples=self.num_examples(dataloader))

        # Finally, turn the aggregated tensors into numpy arrays.
        # if preds is not None:
        #     preds = preds.cpu().numpy()
        if source_tag_preds is not None:
            source_tag_preds = source_tag_preds.cpu().numpy()
        if mt_tag_preds is not None:
            mt_tag_preds = mt_tag_preds.cpu().numpy()
        if label_ids is not None:
            label_ids = label_ids.cpu().numpy()

        metrics = {}
        if len(eval_losses) > 0:
            metrics["eval_loss"] = np.sum(eval_losses) / samples_count

        # Prefix all keys with eval_
        for key in list(metrics.keys()):
            if not key.startswith("eval_"):
                metrics[f"eval_{key}"] = metrics.pop(key)

        return QETagClassificationPredictionOutput(source_tag_predictions=source_tag_preds,
                                                   mt_tag_predictions=mt_tag_preds,
                                                   source_tag_mask=source_tag_mask,
                                                   mt_tag_mask=mt_tag_mask,
                                                   label_ids=label_ids,
                                                   metrics=metrics)

    def prediction_step(
            self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool
    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor],
               Optional[torch.Tensor], Optional[torch.Tensor]]:
        has_labels = any(inputs.get(k) is not None for k in ["labels", "lm_labels", "masked_lm_labels", 'tag_labels'])

        inputs = self._prepare_inputs(inputs)

        with torch.no_grad():
            outputs = model(**inputs)
            source_tag_mask, mt_tag_mask = model.generate_source_and_mt_tag_mask(inputs['token_type_ids'])
            if has_labels:
                loss, logits = outputs[:2]
                source_tag_logits, mt_tag_logits = logits
                loss = loss.mean().item()
            else:
                loss = None
                logits = outputs[0]
                source_tag_logits, mt_tag_logits = logits
            if self.args.past_index >= 0:
                self._past = outputs[self.args.past_index if has_labels else self.args.past_index - 1]

        if prediction_loss_only:
            return (loss, None, None, None, None, None)

        labels = inputs.get("labels")
        if labels is not None:
            labels = labels.detach()
        return (loss, source_tag_logits, mt_tag_logits, source_tag_mask, mt_tag_mask, labels)


class BertModelWithQETag(BertPreTrainedModel):
    '''
    This class is temporarily identical with BertModel
    Considering the possible extension in the future, I re-implement this class here.
    '''

    def __init__(self, config):
        super().__init__(config)
        self.config = config

        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)

        self.init_weights()

    def forward(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            encoder_hidden_states=None,
            encoder_attention_mask=None,
            output_attentions=None,
            output_hidden_states=None,
            return_dict=None,
    ):
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
        elif input_ids is not None:
            input_shape = input_ids.size()
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        device = input_ids.device if input_ids is not None else inputs_embeds.device

        if attention_mask is None:
            attention_mask = torch.ones(input_shape, device=device)
        if token_type_ids is None:
            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)

        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)

        if self.config.is_decoder and encoder_hidden_states is not None:
            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
            if encoder_attention_mask is None:
                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)
        else:
            encoder_extended_attention_mask = None

        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)

        embedding_output = self.embeddings(
            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
        )

        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        sequence_output = encoder_outputs[0]
        pooled_output = self.pooler(sequence_output)

        if not return_dict:
            return (sequence_output, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )


class BertForQETagClassification(BertPreTrainedModel):
    def __init__(self, config, **kwargs):
        super().__init__(config, **kwargs)

        self.bert = BertModelWithQETag(config)

        self.source_qe_tag_outputs = nn.Linear(config.hidden_size, 2)  # OK or BAD
        self.mt_qe_tag_outputs = nn.Linear(config.hidden_size, 2)  # OK or BAD

    def forward(
            self,
            input_ids=None,
            attention_mask=None,
            token_type_ids=None,
            position_ids=None,
            head_mask=None,
            inputs_embeds=None,
            tag_labels=None,
    ):
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds
        )

        sequence_output = outputs[0]

        source_tag_masks, mt_tag_masks = self.generate_source_and_mt_tag_mask(token_type_ids)

        total_loss = None
        source_tag_logits = self.source_qe_tag_outputs(sequence_output)
        mt_tag_logits = self.mt_qe_tag_outputs(sequence_output)
        if tag_labels is not None:
            loss_fct = CrossEntropyLoss()

            source_active_tag_labels = torch.where(source_tag_masks.view(-1), tag_labels.view(-1),
                                                   torch.tensor(loss_fct.ignore_index).type_as(tag_labels))
            source_tag_loss = loss_fct(source_tag_logits.view(-1, 2), source_active_tag_labels)

            mt_active_tag_labels = torch.where(mt_tag_masks.view(-1), tag_labels.view(-1),
                                               torch.tensor(loss_fct.ignore_index).type_as(tag_labels))
            mt_tag_loss = loss_fct(mt_tag_logits.view(-1, 2), mt_active_tag_labels)

            total_loss = source_tag_loss + mt_tag_loss

        output = ((source_tag_logits, mt_tag_logits),) + outputs[2:]
        return ((total_loss,) + output) if total_loss is not None else output

    def generate_source_and_mt_tag_mask(self, token_type_ids):
        '''
        generate masks for source tags and MT tags. Note that CLS and SEP are not included.
        specifically, identifying the min and max indices of 1 in token_type_ids, which respectively identifies the
        first token of MT text and the SEP token of MT text.
        '''
        batch_size, max_len = token_type_ids.shape
        source_tag_masks = []
        mt_tag_masks = []
        for i in range(batch_size):
            row_type_ids = token_type_ids[i, :]
            _mt_indices = row_type_ids.nonzero().view(-1)
            min_i, max_i = _mt_indices.min(), _mt_indices.max()

            source_tag_mask = torch.zeros_like(row_type_ids)
            source_tag_mask[1:min_i - 1] = 1  # CLS and SEP for source text are excluded

            mt_tag_mask = torch.zeros_like(row_type_ids)
            mt_tag_mask[min_i:max_i] = 1  # SEP for MT text is excluded

            source_tag_masks.append(source_tag_mask.type(torch.bool))
            mt_tag_masks.append(mt_tag_mask.type(torch.bool))

        source_tag_masks = torch.stack(source_tag_masks, dim=0)
        mt_tag_masks = torch.stack(mt_tag_masks, dim=0)
        return source_tag_masks, mt_tag_masks


#############################################################
#
#  End
#
#############################################################

@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """
    model_type: str = field(
        metadata={"help": "Type of model"}
    )
    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    # task_type: Optional[str] = field(
    #     default="NER", metadata={"help": "Task type to fine tune in training (e.g. NER, POS, etc)"}
    # )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    use_fast: bool = field(default=False, metadata={"help": "Set this flag to use fast tokenization."})

    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )


@dataclass
class DataTrainingArguments:
    source_text: str = field(
        metadata={'help': 'Path to the source text file.'}
    )
    mt_text: str = field(
        metadata={'help': 'Path to the MT text file.'}
    )
    source_qe_tags: str = field(
        default=None,
        metadata={'help': 'Path to the source QE tags file.'}
    )
    mt_qe_tags: str = field(
        default=None,
        metadata={'help': 'Path to the MT QE tags file.'}
    )
    max_seq_length: int = field(
        default=128,
        metadata={
            "help": "The maximum total input sequence length after tokenization. Sequences longer "
                    "than this will be truncated, sequences shorter will be padded."
        },
    )
    do_lower_case: bool = field(
        default=False,
        metadata={'help': 'Set this flag if you are using an uncased model.'}
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )


def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))

    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,
    )
    logger.warning(
        "Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s",
        training_args.local_rank,
        training_args.device,
        training_args.n_gpu,
        bool(training_args.local_rank != -1),
        training_args.fp16,
    )
    logger.info("Training/evaluation parameters %s", training_args)

    # Set seed
    set_seed(training_args.seed)

    # labels = token_classification_task.get_labels(data_args.labels)
    labels = ['BAD', 'OK']
    id_to_label: Dict[int, str] = {i: label for i, label in enumerate(labels)}
    label_to_id: Dict[str, int] = {label: i for i, label in enumerate(labels)}
    num_labels = len(labels)

    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        num_labels=num_labels,
        id2label=id_to_label,
        label2id={label: i for i, label in enumerate(labels)},
        cache_dir=model_args.cache_dir,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        do_lower_case=data_args.do_lower_case,
        cache_dir=model_args.cache_dir,
        use_fast=model_args.use_fast,
    )
    model = BertForQETagClassification.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
    )

    # Get datasets
    train_dataset = (
        QETagClassificationDataset(args=data_args, tokenizer=tokenizer, set_type='train', label_to_id=label_to_id)
        if training_args.do_train
        else None
    )
    eval_dataset = (
        QETagClassificationDataset(args=data_args, tokenizer=tokenizer, set_type='eval', label_to_id=label_to_id)
        if training_args.do_eval
        else None
    )

    def align_predictions(predictions: np.ndarray, mask: torch.Tensor) -> List[List[str]]:
        preds = np.argmax(predictions, axis=2)
        batch_size, max_len = preds.shape
        res = [[] for _ in range(batch_size)]

        mask = mask.cpu().numpy()
        preds[~mask] = -100

        for i in range(batch_size):
            for j in range(1, max_len):
                if preds[i, j] >= 0:
                    res[i].append(preds[i, j])
                elif preds[i, j - 1] >= 0:
                    break

        return res

    def map_tag_to_origin(text, tokenizer, tags):
        pieced_to_origin_map = map_offset(text, tokenizer)
        assert max(pieced_to_origin_map.keys()) == len(tags) - 1, f'Inconsistent num of tokens in case:\n{text}\n{tags}'
        new_tags = collections.defaultdict(list)
        for i, tag in enumerate(tags):
            new_tags[pieced_to_origin_map[i]].append(tag)

        res = []
        for i in sorted(new_tags):
            c = collections.Counter(new_tags[i])
            res.append(c.most_common(1)[0][0])

        assert len(res) == len(text.split())
        return res

    # Initialize our Trainer
    trainer = QETagClassificationTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    # Training
    if training_args.do_train:
        trainer.train(
            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None
        )
        trainer.save_model()
        if trainer.is_world_master():
            tokenizer.save_pretrained(training_args.output_dir)

    # Predict
    if training_args.do_eval:
        test_dataset = eval_dataset

        source_tag_predictions, mt_tag_predictions, source_tag_mask, mt_tag_mask, label_ids, metrics = \
            trainer.predict(test_dataset)
        source_tag_preds = align_predictions(source_tag_predictions, source_tag_mask)
        mt_tag_preds = align_predictions(mt_tag_predictions, mt_tag_mask)

        orig_source_tag_preds = []
        with Path(data_args.source_text).open(encoding='utf-8') as f:
            src_lines = [l.strip() for l in f]
            for src_line, source_tag_pred in zip(src_lines, source_tag_preds):
                orig_source_tag_preds.append(map_tag_to_origin(src_line, tokenizer, source_tag_pred))

        orig_mt_tag_preds = []
        with Path(data_args.mt_text).open(encoding='utf-8') as f:
            mt_lines = [l.strip() for l in f]
            for mt_line, mt_tag_pred in zip(mt_lines, mt_tag_preds):
                orig_mt_tag_preds.append(map_tag_to_origin(mt_line, tokenizer, mt_tag_pred))

        source_tag_res_file = os.path.join(training_args.output_dir, 'pred.source_tags')
        mt_tag_res_file = os.path.join(training_args.output_dir, 'pred.mtword_tags')

        if trainer.is_world_master():
            with Path(source_tag_res_file).open('w') as f:
                for tags in orig_source_tag_preds:
                    f.write(' '.join(id_to_label[t] for t in tags) + '\n')

            with Path(mt_tag_res_file).open('w') as f:
                for tags in orig_mt_tag_preds:
                    f.write(' '.join(id_to_label[t] for t in tags) + '\n')


if __name__ == "__main__":
    main()
